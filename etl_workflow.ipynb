{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7b52809",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c5f6968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File System & Utilities\n",
    "# ----------------------\n",
    "import os\n",
    "import zipfile\n",
    "import getpass\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from utils import get_latest_zip # utils.py\n",
    "from typing import Tuple, Optional, Callable\n",
    "# import ftplib\n",
    "# import tempfile\n",
    "# from io import BytesIO\n",
    "\n",
    "# Data Manipulation & time\n",
    "# -------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "# FTP server communication\n",
    "# -------------------------\n",
    "import paramiko\n",
    "\n",
    "# Data Validation\n",
    "# ----------------\n",
    "import pandera as pa\n",
    "from pandera.typing.pandas import Series\n",
    "\n",
    "# For workflow automation\n",
    "# -----------------------\n",
    "from prefect import task, flow, get_run_logger # type: ignore\n",
    "\n",
    "# Ignore warnings\n",
    "# ----------------\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef4d84e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Eddie\\OneDrive - eRoute2Market\\eRoute2Market\\Agents\\etl-automation-with-prefect\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Print current working directory\n",
    "print(os.getcwd()) # os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5f6968",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cb4bd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data():\n",
    "    \"\"\"\n",
    "    Connects to SFTP and downloads Vilbev-{YYYYMMDD}.zip after removing\n",
    "    any existing Vilbev-*.zip files in ./data/raw.\n",
    "    \"\"\"\n",
    "    current_date = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "    # ---- PATHS ----\n",
    "    data_dir = Path(\"./data/raw\")\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    local_file = data_dir / f\"Vilbev-{current_date}.zip\"\n",
    "    remote_file = f\"/home/viljoenbev/Vilbev-{current_date}.zip\"\n",
    "\n",
    "    # ---- DELETE LOCAL Vilbev FILES FIRST ----\n",
    "    print(\"ðŸ§¹ Cleaning up existing Vilbev-*.zip files in ./data/raw ...\")\n",
    "    deleted_any = False\n",
    "    for p in data_dir.glob(\"Vilbev-*.zip\"):\n",
    "        try:\n",
    "            p.unlink()\n",
    "            deleted_any = True\n",
    "            print(f\"ðŸ—‘ï¸ Deleted: {p.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error deleting {p.name}: {e}\")\n",
    "    if not deleted_any:\n",
    "        print(\"â„¹ï¸ No existing Vilbev-*.zip files found to delete.\")\n",
    "\n",
    "    # (Optional) ensure target file does not existâ€”even if name pattern changes in future\n",
    "    if local_file.exists():\n",
    "        try:\n",
    "            local_file.unlink()\n",
    "            print(f\"ðŸ—‘ï¸ Removed pre-existing target file: {local_file.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error deleting pre-existing target file {local_file.name}: {e}\")\n",
    "\n",
    "    # ---- PARAMIKO CLIENT SETUP ----\n",
    "    client = paramiko.SSHClient()\n",
    "    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "\n",
    "    sftp = None\n",
    "    try:\n",
    "        # ---- CONNECT ----\n",
    "        host = os.getenv('ftp_host')\n",
    "        port = int(os.getenv('ftp_port'))  # ensure integer\n",
    "        user = os.getenv('ftp_user')\n",
    "        pwd  = os.getenv('ftp_pass')\n",
    "\n",
    "        client.connect(\n",
    "            hostname=host,\n",
    "            port=port,\n",
    "            username=user,\n",
    "            password=pwd,\n",
    "            allow_agent=False,\n",
    "            look_for_keys=False,\n",
    "            timeout=30,\n",
    "        )\n",
    "        sftp = client.open_sftp()\n",
    "        print(\"ðŸ” Connected to SFTP server\")\n",
    "\n",
    "        # ---- DOWNLOAD ----\n",
    "        print(f\"ðŸ“¥ Downloading: {remote_file} â†’ {local_file}\")\n",
    "        sftp.get(\n",
    "            remotepath=remote_file,\n",
    "            localpath=str(local_file),\n",
    "            callback=None  # add progress callback if you need it\n",
    "        )\n",
    "        print(\"âœ… Download complete!\")\n",
    "\n",
    "        print(f\"ðŸ“ File saved on {str(local_file)}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ Remote file not found: {remote_file}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during SFTP operation: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # ---- CLEAN UP ----\n",
    "        try:\n",
    "            if sftp is not None:\n",
    "                sftp.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            client.close()\n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "756f6369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaning up existing Vilbev-*.zip files in ./data/raw ...\n",
      "ðŸ—‘ï¸ Deleted: Vilbev-20260202.zip\n",
      "ðŸ” Connected to SFTP server\n",
      "ðŸ“¥ Downloading: /home/viljoenbev/Vilbev-20260202.zip â†’ data\\raw\\Vilbev-20260202.zip\n",
      "âœ… Download complete!\n",
      "ðŸ“ File saved on data\\raw\\Vilbev-20260202.zip\n"
     ]
    }
   ],
   "source": [
    "download_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b8925e",
   "metadata": {},
   "source": [
    "## Unzip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c71c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract the first CSV file from a ZIP archive and load it into a pandas DataFrame.\n",
    "    Handles:\n",
    "    - file existence checks\n",
    "    - multiple CSV files (selects first match)\n",
    "    - safe extraction into a temp folder\n",
    "    - consistent return behavior\n",
    "    \"\"\"\n",
    "    zip_file_path = get_latest_zip(os.getenv('BASE_DIR'))\n",
    "\n",
    "    if not os.path.exists(zip_file_path):\n",
    "        raise FileNotFoundError(f\"âŒ ZIP file does not exist: {zip_file_path}\")\n",
    "\n",
    "    print(\"ðŸ“¦ Reading ZIP archive!\")\n",
    "\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "\n",
    "        # List all files\n",
    "        file_list = zip_ref.namelist()\n",
    "        print(\"ðŸ“ Files inside ZIP:\", file_list)\n",
    "\n",
    "        # find CSV file(s)\n",
    "        csv_files = [f for f in file_list if f.lower().endswith(\".csv\")]\n",
    "\n",
    "        if not csv_files:\n",
    "            raise ValueError(\"âŒ No CSV file found inside ZIP.\")\n",
    "\n",
    "        # Use the first CSV file found\n",
    "        csv_file_name = csv_files[0]\n",
    "        print(f\"ðŸ“„ Found CSV file: {csv_file_name}\")\n",
    "\n",
    "        # Ensure extraction directory exists\n",
    "        extract_dir = \"data\"\n",
    "        os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "        # Extract file (optional but useful for debugging)\n",
    "        extracted_path = zip_ref.extract(csv_file_name, path=extract_dir)\n",
    "        print(f\"ðŸ“¤ Extracted to: {extracted_path}\")\n",
    "\n",
    "        # Load CSV into pandas directly from ZIP\n",
    "        with zip_ref.open(csv_file_name) as csv_file:\n",
    "            try:\n",
    "                df = pd.read_csv(csv_file)\n",
    "                print(f\"âœ… Loaded CSV: {csv_file_name}\")\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"âŒ Failed to read CSV inside ZIP: {e}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a51ba691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Latest ZIP selected: C:\\Users\\Eddie\\OneDrive - eRoute2Market\\eRoute2Market\\Agents\\etl-automation-with-prefect\\data\\raw\\Vilbev-20260202.zip\n",
      "ðŸ“¦ Reading ZIP archive!\n",
      "ðŸ“ Files inside ZIP: ['Vilbev-20260202.csv']\n",
      "ðŸ“„ Found CSV file: Vilbev-20260202.csv\n",
      "ðŸ“¤ Extracted to: data\\Vilbev-20260202.csv\n",
      "âœ… Loaded CSV: Vilbev-20260202.csv\n"
     ]
    }
   ],
   "source": [
    "raw = extract_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942b2977",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c7fdc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to transform Viljoen Beverages data\n",
    "\n",
    "    Args:\n",
    "        df: Input dataframe to transform\n",
    "        returns: Transformed dataframe\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Standard column layout\n",
    "    columns=[\n",
    "        'SellerID','GUID','Date','Reference','Customer_Code','Name','Physical_Address1',\\\n",
    "        'Physical_Address2','Physical_Address3','Physical_Address4','Telephone',\\\n",
    "        'Stock_Code','Description','Price_Ex_Vat','Quantity','RepCode','ProductBarCodeID'\n",
    "        ]\n",
    "    # Create an empty dataframe\n",
    "    df1=pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Build the dataframe\n",
    "    df1['Date']=df['Date']\n",
    "    df1['SellerID']='VILJOEN'\n",
    "    df1['GUID']=0\n",
    "    df1['Reference']=df['Reference']\n",
    "    df1['Customer_Code']=df['Customer code']\n",
    "    df1['Name']=df['Customer name']\n",
    "    df1['Physical_Address1']=df['Physical_Address1']\n",
    "    df1['Physical_Address2']=df['Physical_Address2']\n",
    "    df1['Physical_Address3']=df['Physical_Address3']\n",
    "    df1['Physical_Address4']=(\n",
    "        df['Deliver1'].fillna('').astype(str) +' '+\n",
    "        df['Deliver2'].fillna('').astype(str) +' '+\n",
    "        df['Deliver3'].fillna('').astype(str) +' '+\n",
    "        df['Deliver4'].fillna('').astype(str)\n",
    "        ).str.strip()\n",
    "\n",
    "    df1['Telephone']=df['Telephone']\n",
    "    df1['Stock_Code']=df['Product code']\n",
    "    df1['Description']=df['Product description']\n",
    "    df1['Price_Ex_Vat']=round(abs(df['Value']/df['Quantity']),2)\n",
    "    df1['Quantity']=df['Quantity']\n",
    "    df1['RepCode']=df['Rep']\n",
    "    df1['ProductBarCodeID']=''\n",
    "\n",
    "    print(f\"â„¹ï¸ Total quantity: {np.sum(df1['Quantity']):.0f}\\n\")\n",
    "\n",
    "    df2=df1.copy()\n",
    "    df2['Date']=pd.to_datetime(df2['Date'])\n",
    "    df2['Date']=df2['Date'].apply(lambda x: x.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "    df1[\"Name\"].fillna('SPAR NORTH RAND (11691)', inplace=True)\n",
    "    #   DATE FORMAT CLEANING\n",
    "    # -----------------------------\n",
    "    print(\"âœ… Date fomat cleaned\")\n",
    "    df1['Date'] = pd.to_datetime(df1['Date'], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
    "    print(\"âœ… Data transformation complete!\")\n",
    "\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4022f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ Total quantity: 739\n",
      "\n",
      "âœ… Date fomat cleaned\n",
      "âœ… Data transformation complete!\n"
     ]
    }
   ],
   "source": [
    "df = transform_data(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82a6483",
   "metadata": {},
   "source": [
    "## Validate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bcdb015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Function to validate data\n",
    "    \"\"\"\n",
    "    # logger = get_run_logger()\n",
    "    class Schema(pa.DataFrameModel):\n",
    "        # 1. Check data types and uniqueness\n",
    "        SellerID: Series[str] = pa.Field(nullable=False)  # seller IDs must be non-null\n",
    "        GUID: Series[int] = pa.Field(ge=0, nullable=False)  # must be non-null\n",
    "\n",
    "        # 2. Dates coerced to proper datetime\n",
    "        Date: Series[pd.Timestamp] = pa.Field(coerce=False, nullable=False) # must be non-null\n",
    "\n",
    "        # 3. Reference and customer codes\n",
    "        Reference: Series[str] = pa.Field(nullable=False) # must be non-null\n",
    "        Customer_Code: Series[str] = pa.Field(str_matches=r\"^[A-Z0-9]+$\", nullable=False)  # must be non-null\n",
    "\n",
    "        # 4. Customer details\n",
    "        Name: Series[str] = pa.Field(nullable=False) # must be non-null\n",
    "        Physical_Address1: Series[str] = pa.Field(nullable=True)\n",
    "        Physical_Address2: Series[str] = pa.Field(nullable=True)\n",
    "        Physical_Address3: Series[str] = pa.Field(nullable=True)\n",
    "        Physical_Address4: Series[str] = pa.Field(nullable=True)\n",
    "\n",
    "        # 5. Telephone validation (basic regex for digits, spaces, +, -)\n",
    "        Telephone: Series[str] = pa.Field(nullable=True)\n",
    "\n",
    "        # 6. Product details\n",
    "        Stock_Code: Series[str] = pa.Field(nullable=False) # must be non-null\n",
    "        Description: Series[str] = pa.Field(nullable=False) # must be non-null\n",
    "        Price_Ex_Vat: Series[float] = pa.Field(ge=0.0, nullable=False)  # must be non-null\n",
    "        Quantity: Series[int] = pa.Field(nullable=False)  # must be non-null\n",
    "\n",
    "        # 7. Rep and barcode\n",
    "        RepCode: Series[str] = pa.Field(nullable=True)\n",
    "        ProductBarCodeID: Series[str] = pa.Field(nullable=True)  # typical EAN/UPC\n",
    "\n",
    "        class Config:\n",
    "            strict = True  # enforce exact schema\n",
    "            coerce = True  # auto-convert types where possible\n",
    "\n",
    "    try:\n",
    "        # lazy=True means \"find all errors before crashing\"\n",
    "        Schema.validate(df, lazy=True)\n",
    "        print(\"âœ… Data passed validation!\\nâ„¹ï¸ Proceeding to next step.\")\n",
    "\n",
    "    except pa.errors.SchemaErrors as err:\n",
    "        print(\"âš ï¸ Data Contract Breached!.......\\n\")\n",
    "        print(f\"âŒ Total errors found: {len(err.failure_cases)}\")\n",
    "\n",
    "        # Let's look at the specific failures\n",
    "        print(\"\\n*********âš ï¸Failure Reportâš ï¸************\\n\")\n",
    "        print(err.failure_cases[['column', 'check', 'failure_case']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bcac4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data passed validation!\n",
      "â„¹ï¸ Proceeding to next step.\n"
     ]
    }
   ],
   "source": [
    "validate_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e5fb35",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a31130b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Validate date in cleaned file\n",
    "# -------------------------------------------------------------\n",
    "def validate_dates(\n",
    "    min_date: pd.Timestamp,\n",
    "    max_date: pd.Timestamp,\n",
    "    today: datetime = None,\n",
    "    lookback_days: int = 3) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Raises ValueError if the date range is not entirely within the last `lookback_days`\n",
    "    and if the latest month is neither the current month nor the previous month.\n",
    "    \"\"\"\n",
    "\n",
    "    if today is None:\n",
    "        today = datetime.now()\n",
    "\n",
    "    # Normalize to date (drop time)\n",
    "    today_d = today.date()\n",
    "    window_start = today_d - timedelta(days=lookback_days)\n",
    "\n",
    "    min_d = min_date.date()\n",
    "    max_d = max_date.date()\n",
    "\n",
    "    # 1) Entire range must be within the last `lookback_days` days (inclusive)\n",
    "    if not (window_start <= min_d <= today_d and window_start <= max_d <= today_d):\n",
    "        raise ValueError(\n",
    "            f\"âŒ Date range {min_d} to {max_d} is not fully within the last {lookback_days} days \"\n",
    "            f\"({window_start}..{today_d}).\"\n",
    "        )\n",
    "\n",
    "    # 2) Month check on the latest date in the file (max_d)\n",
    "    cur_month = today_d.month\n",
    "    prev_month = 12 if cur_month == 1 else cur_month - 1\n",
    "    file_month = max_d.month\n",
    "\n",
    "    if file_month not in (cur_month, prev_month):\n",
    "        raise ValueError(\n",
    "            f\"âŒ Latest file month ({file_month}) is not the current month ({cur_month}) \"\n",
    "            f\"or previous month ({prev_month}).\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_local(\n",
    "    df: pd.DataFrame,\n",
    "    create_dir_if_missing: bool = True,\n",
    "    delete_existing_csvs: bool = True,        # â† new flag to control cleanup\n",
    "    restrict_delete_to_prefix: str | None = None  # e.g., \"Viljoenbev_\" to only delete those CSVs\n",
    ") -> Tuple[str, bool]:\n",
    "    \"\"\"\n",
    "    Save cleaned data to a CSV inside the folder specified by OUTPUT_DIR in .env,\n",
    "    only if:\n",
    "    - the DataFrame's date range is entirely within the last 3 days, and\n",
    "    - the latest date's month is the current month or the previous month.\n",
    "    Skips save if a file with the same name already exists.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (full_path, saved) : Tuple[str, bool]\n",
    "        full_path -> absolute path to the intended CSV\n",
    "        saved     -> True if file was written, False if skipped (already existed)\n",
    "    \"\"\"\n",
    "    # --- Resolve OUTPUT_DIR ---\n",
    "    output_dir = os.getenv(\"OUTPUT_DIR\")\n",
    "    if not output_dir:\n",
    "        raise ValueError(\"Environment variable 'OUTPUT_DIR' is not set in your environment or .env file.\")\n",
    "\n",
    "    output_dir_path = Path(os.path.abspath(os.path.expanduser(output_dir)))\n",
    "    if not output_dir_path.is_dir():\n",
    "        if create_dir_if_missing:\n",
    "            output_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "            print(f\"ðŸ“ Created output directory: {output_dir_path}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Output directory does not exist: {output_dir_path}\")\n",
    "\n",
    "    # ---- DELETE EXISTING CSVs IN CLEANED FOLDER (before saving) ----\n",
    "    if delete_existing_csvs:\n",
    "        # Choose the pattern:\n",
    "        #   \"*.csv\" to delete ALL CSVs in the folder\n",
    "        #   f\"{restrict_delete_to_prefix}*.csv\" to only delete those starting with a prefix\n",
    "        if restrict_delete_to_prefix:\n",
    "            pattern = f\"{restrict_delete_to_prefix}*.csv\"\n",
    "        else:\n",
    "            pattern = \"*.csv\"\n",
    "\n",
    "        print(f\"ðŸ§¹ Cleaning up existing CSV file in:\\nðŸ“ {output_dir_path}.\")\n",
    "        deleted_any = False\n",
    "        for p in output_dir_path.glob(pattern):\n",
    "            try:\n",
    "                p.unlink()\n",
    "                deleted_any = True\n",
    "                print(f\"ðŸ—‘ï¸ Deleted CSV: {p.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error deleting {p.name}: {e}\")\n",
    "        if not deleted_any:\n",
    "            print(\"â„¹ï¸ No matching CSV files found to delete.\")\n",
    "\n",
    "    # --- Prepare and validate dates ---\n",
    "    if \"Date\" not in df.columns:\n",
    "        raise KeyError(\"Input DataFrame must contain a 'Date' column.\")\n",
    "\n",
    "    data = df.copy()\n",
    "    data[\"Date\"] = pd.to_datetime(data[\"Date\"], errors=\"coerce\")\n",
    "\n",
    "    if data[\"Date\"].isna().all():\n",
    "        raise ValueError(\"All values in 'Date' are NaT after parsing. Check your input data.\")\n",
    "\n",
    "    min_date = data[\"Date\"].dropna().min()\n",
    "    max_date = data[\"Date\"].dropna().max()\n",
    "\n",
    "    # Validation per your rule:\n",
    "    validate_dates(min_date, max_date, lookback_days=3)\n",
    "\n",
    "    # --- Build deterministic filename and check for existence ---\n",
    "    min_str = min_date.strftime(\"%Y-%m-%d\")\n",
    "    max_str = max_date.strftime(\"%Y-%m-%d\")\n",
    "    filename = f\"Viljoenbev_{min_str}_to_{max_str}.csv\"\n",
    "    full_path = output_dir_path / filename\n",
    "\n",
    "    if full_path.exists():\n",
    "        print(f\"ðŸ›‘ File already exists, skipping save:\\nðŸ“ {full_path}\")\n",
    "        return str(full_path), False\n",
    "\n",
    "    # --- Finalize and save ---\n",
    "    data[\"Date\"] = data[\"Date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "    data.to_csv(full_path, index=False)\n",
    "    print(f\"\\nâœ… Data saved to:\\nðŸ“ {full_path}\")\n",
    "    return str(full_path), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66c94f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaning up existing CSV file in:\n",
      "ðŸ“ c:\\Users\\Eddie\\OneDrive - eRoute2Market\\eRoute2Market\\Agents\\etl-automation-with-prefect\\data\\cleaned.\n",
      "â„¹ï¸ No matching CSV files found to delete.\n",
      "\n",
      "âœ… Data saved to:\n",
      "ðŸ“ c:\\Users\\Eddie\\OneDrive - eRoute2Market\\eRoute2Market\\Agents\\etl-automation-with-prefect\\data\\cleaned\\Viljoenbev_2026-01-30_to_2026-01-30.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('c:\\\\Users\\\\Eddie\\\\OneDrive - eRoute2Market\\\\eRoute2Market\\\\Agents\\\\etl-automation-with-prefect\\\\data\\\\cleaned\\\\Viljoenbev_2026-01-30_to_2026-01-30.csv',\n",
       " True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_to_local(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9132f0",
   "metadata": {},
   "source": [
    "## Upload to FTP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e1d3052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_server(csv_file_path: str = None,) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Upload a specific CSV file to an SFTP server using only paramiko.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    csv_file_path : str\n",
    "        Path to the local CSV file to upload. If None, it will pick the first\n",
    "        file matching 'Viljoenbev_*.csv' in `output_dir`.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str or None\n",
    "        Remote path where the file was uploaded, or None if the upload failed.\n",
    "    \"\"\"\n",
    "    # ---- CONNECT ----\n",
    "    sftp_host = os.getenv('ftp_host')\n",
    "    sftp_port = int(os.getenv('ftp_port'))  # ensure integer\n",
    "    sftp_user = os.getenv('ftp_user')\n",
    "    sftp_pass  = os.getenv('ftp_pass')\n",
    "\n",
    "    remote_dir = os.getenv('ftp_dir')\n",
    "\n",
    "    # Fallback to discover a file if not provided (mirrors your original default idea)\n",
    "    if csv_file_path is None:\n",
    "        # Adjust `output_dir` to your actual variable/scope if needed\n",
    "        output_dir = os.getenv('OUTPUT_DIR')\n",
    "        matches = glob(os.path.join(output_dir, 'Viljoenbev_*.csv'))\n",
    "        if not matches:\n",
    "            print(\"âš ï¸ No CSV file found matching 'Viljoenbev_*.csv'.\")\n",
    "            return None\n",
    "        csv_file_path = matches[0]\n",
    "\n",
    "    try:\n",
    "        # Verify the local file exists\n",
    "        if not os.path.exists(csv_file_path):\n",
    "            print(f\"âš ï¸ Local file not found: {csv_file_path}\")\n",
    "            return None\n",
    "\n",
    "        filename = os.path.basename(csv_file_path)\n",
    "        # Normalize remote path to POSIX style for SFTP\n",
    "        remote_dir_posix = remote_dir.replace('\\\\', '/').rstrip('/') + '/'\n",
    "        remote_path = (remote_dir_posix + filename)\n",
    "\n",
    "        # Create SSH client and connect\n",
    "        ssh = paramiko.SSHClient()\n",
    "\n",
    "        # WARNING: Auto-adding host keys reduces security. Prefer loading known hosts in production.\n",
    "        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "\n",
    "        print(f\"ðŸ” Connecting to FTP server as {sftp_user}.\")\n",
    "        ssh.connect(\n",
    "            hostname=sftp_host,\n",
    "            port=sftp_port,\n",
    "            username=sftp_user,\n",
    "            password=sftp_pass,\n",
    "            look_for_keys=False,\n",
    "            allow_agent=False,\n",
    "            timeout=30\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            # Open SFTP session\n",
    "            sftp = ssh.open_sftp()\n",
    "\n",
    "            # Ensure remote directory exists (create recursively if missing)\n",
    "            _ensure_remote_dir(sftp, remote_dir_posix)\n",
    "\n",
    "            # Upload with confirmation via file size/stat check\n",
    "            print(f\"ðŸš€ Uploading {filename} to {remote_path}...\")\n",
    "            sftp.put(csv_file_path, remote_path)\n",
    "\n",
    "            # Optional: verify upload completed by checking remote file size\n",
    "            local_size = os.path.getsize(csv_file_path)\n",
    "            remote_stat = sftp.stat(remote_path)\n",
    "            if remote_stat.st_size != local_size:\n",
    "                print(\"âš ï¸ Size mismatch after upload. Upload may be incomplete.\")\n",
    "                return None\n",
    "\n",
    "            print(\"âœ… Upload completed successfully!\")\n",
    "            return None\n",
    "\n",
    "        finally:\n",
    "            try:\n",
    "                sftp.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            ssh.close()\n",
    "\n",
    "    except paramiko.AuthenticationException:\n",
    "        print(\"âš ï¸ Authentication failed. Please verify username/password (or key).\")\n",
    "        return None\n",
    "    except paramiko.SSHException as e:\n",
    "        print(f\"âŒ SSH/SFTP error: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error uploading file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Remote Directory Helpers\n",
    "# ------------------------------\n",
    "def _ensure_remote_dir(sftp: paramiko.SFTPClient, remote_dir_posix: str) -> None:\n",
    "    \"\"\"\n",
    "    Recursively create remote directories if they do not exist.\n",
    "    `remote_dir_posix` must be a POSIX-style path ending with '/'.\n",
    "    \"\"\"\n",
    "    # Split path into components and build progressively\n",
    "    # Handle absolute paths like '/home/user/data/'\n",
    "    parts = [p for p in remote_dir_posix.split('/') if p]\n",
    "    prefix = '/' if remote_dir_posix.startswith('/') else ''\n",
    "\n",
    "    current = prefix\n",
    "    for part in parts:\n",
    "        current = (current.rstrip('/') + '/' + part)\n",
    "        try:\n",
    "            sftp.stat(current)  # Exists\n",
    "        except FileNotFoundError:\n",
    "            sftp.mkdir(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c449549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Connecting to FTP server as viljoenbev.\n",
      "ðŸš€ Uploading Viljoenbev_2026-01-30_to_2026-01-30.csv to /home/viljoenbev/data/Viljoenbev_2026-01-30_to_2026-01-30.csv...\n",
      "âœ… Upload completed successfully!\n"
     ]
    }
   ],
   "source": [
    "upload_to_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edb18fe",
   "metadata": {},
   "source": [
    "## Run import Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90a214df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_import(timeout: int = 120):\n",
    "    \"\"\"\n",
    "    Connect to a remote server via SSH and execute commands, first testing then running.\n",
    "\n",
    "    Args:\n",
    "        timeout (int): Timeout for command execution in seconds\n",
    "\n",
    "    Returns:\n",
    "        tuple: (stdout, stderr, exit_code) from the last command executed\n",
    "    \"\"\"\n",
    "    # Access login credentials\n",
    "    hostname = os.getenv('host_name')\n",
    "    port = int(os.getenv('port'))\n",
    "    username = os.getenv('user_name')\n",
    "    password = os.getenv('password')\n",
    "\n",
    "    # Define commands\n",
    "    commands = {\n",
    "        'test': \"/usr/local/eroute2market/supply_chain/scripts/importtxns.pl /home/viljoenbev/data 1\",\n",
    "        'run': \"/usr/local/eroute2market/supply_chain/scripts/importtxns.pl /home/viljoenbev/data\"\n",
    "    }\n",
    "\n",
    "    # If no password provided, prompt for it securely\n",
    "    if password is None:\n",
    "        password = getpass.getpass(f\"Enter SSH password for {username}@{hostname}: \")\n",
    "\n",
    "    # Create an SSH client instance\n",
    "    client = paramiko.SSHClient()\n",
    "\n",
    "    try:\n",
    "        # Automatically add the server's host key\n",
    "        client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "\n",
    "        # Connect to the remote server\n",
    "        print(f\"Connecting to {hostname} on port {port}...\")\n",
    "        client.connect(\n",
    "            hostname=hostname,\n",
    "            port=port,\n",
    "            username=username,\n",
    "            password=password\n",
    "        )\n",
    "\n",
    "        # First execute the test command\n",
    "        print(f\"Executing test command:>>>>>>>> {commands['test']}\")\n",
    "        stdin, stdout, stderr = client.exec_command(commands['test'], timeout=timeout)\n",
    "        exit_status = stdout.channel.recv_exit_status()\n",
    "        stdout_str = stdout.read().decode('utf-8')\n",
    "        stderr_str = stderr.read().decode('utf-8')\n",
    "\n",
    "        if exit_status != 0:\n",
    "            print(f\"Test command failed with exit code: {exit_status}\")\n",
    "            print(\"Aborting - not running the main command.\")\n",
    "            return stdout_str, stderr_str, exit_status\n",
    "\n",
    "        print(\"Test command succeeded. \\nNow executing run command...\")\n",
    "\n",
    "        # If test succeeded, execute the run command\n",
    "        print(f\"Executing run command:>>>>>>>> {commands['run']}\")\n",
    "        stdin, stdout, stderr = client.exec_command(commands['run'], timeout=timeout)\n",
    "        exit_status = stdout.channel.recv_exit_status()\n",
    "        stdout_str = stdout.read().decode('utf-8')\n",
    "        stderr_str = stderr.read().decode('utf-8')\n",
    "\n",
    "        # Print status\n",
    "        if exit_status == 0:\n",
    "            print(\"Run command executed successfully.\")\n",
    "        else:\n",
    "            print(f\"Run command failed with exit code: {exit_status}\")\n",
    "\n",
    "        # Return results from the run command\n",
    "        return stdout_str, stderr_str, exit_status\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return \"\", str(e), -1\n",
    "\n",
    "    finally:\n",
    "        # Always close the connection\n",
    "        client.close()\n",
    "        print(\"SSH connection closed.\")\n",
    "\n",
    "# Helper Function\n",
    "# -----------------\n",
    "def parse_perl_output(stdout: str, stderr: str, exit_code: int) -> dict:\n",
    "    working_messages = [line for line in stdout.splitlines() if line.strip()]\n",
    "\n",
    "    stderr_lines = [line for line in stderr.splitlines() if line.strip()]\n",
    "    warnings = [warn for warn in stderr_lines if \"warning\" in warn.lower()]\n",
    "    errors = [err for err in stderr_lines if \"error\" in err.lower()]\n",
    "    other_stderr = [serr for serr in stderr_lines if serr not in warnings + errors]\n",
    "\n",
    "    return {\n",
    "        \"working_on\": working_messages,\n",
    "        \"warnings\": warnings,\n",
    "        \"errors\": errors,\n",
    "        \"other_stderr\": other_stderr,\n",
    "        \"exit_code\": exit_code\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32f41713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to eroute2market.co.za on port 28...\n",
      "Executing test command:>>>>>>>> /usr/local/eroute2market/supply_chain/scripts/importtxns.pl /home/viljoenbev/data 1\n",
      "Test command succeeded. \n",
      "Now executing run command...\n",
      "Executing run command:>>>>>>>> /usr/local/eroute2market/supply_chain/scripts/importtxns.pl /home/viljoenbev/data\n",
      "Run command executed successfully.\n",
      "SSH connection closed.\n",
      "working_on: ['Working on /home/viljoenbev/data/Viljoenbev_2026-01-30_to_2026-01-30.csv', 'Date from 2026-01-30 to 2026-01-30', 'Renaming ...']\n",
      "warnings: ['WARNING: MYSQL_OPT_RECONNECT is deprecated and will be removed in a future version.', 'WARNING: MYSQL_OPT_RECONNECT is deprecated and will be removed in a future version.']\n",
      "errors: []\n",
      "other_stderr: []\n",
      "exit_code: 0\n"
     ]
    }
   ],
   "source": [
    "out, err, cod = run_import()\n",
    "result = parse_perl_output(out, err, cod)\n",
    "for key, value in result.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

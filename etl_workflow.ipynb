{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7b52809",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "b45261c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "8c5f6968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "os.getcwd()\n",
    "import ftplib\n",
    "import tempfile\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "import paramiko\n",
    "import pandera as pa\n",
    "from pandera.typing.pandas import Index, DataFrame, Series\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from datetime import date, timedelta, datetime\n",
    "import time\n",
    "from typing import Tuple, Dict, List, Optional, Iterable, Callable, TypeVar, Any\n",
    "\n",
    "from prefect import task, flow, get_run_logger # type: ignore\n",
    "\n",
    "from utils import get_latest_zip\n",
    "from glob import glob\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202b24d8",
   "metadata": {},
   "source": [
    "## Progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "0ba62c4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipywidgets'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[219]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mipywidgets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IntProgress\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[32m10\u001b[39m), desc=\u001b[33m\"\u001b[39m\u001b[33mDownloading data\u001b[39m\u001b[33m\"\u001b[39m, bar_format=\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{desc}\u001b[39;00m\u001b[33m: | \u001b[39m\u001b[38;5;132;01m{percentage:3.0f}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      3\u001b[39m     time.sleep(\u001b[32m0.5\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'ipywidgets'"
     ]
    }
   ],
   "source": [
    "from ipywidgets import IntProgress\n",
    "for i in tqdm(range(10), desc=\"Downloading data\", bar_format=\"{desc}: | {percentage:3.0f}%\"):\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc18a6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(*, progress=None) -> List[str]:\n",
    "    sftpHost = os.getenv('ftp_host')\n",
    "    sftpPort = int(os.getenv('ftp_port'))\n",
    "    uname = os.getenv('ftp_user')\n",
    "    pwd = os.getenv('ftp_pass')\n",
    "\n",
    "    current_date = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "    # ---- PARAMIKO CLIENT SETUP (replaces pysftp.CnOpts) ----\n",
    "    client = paramiko.SSHClient()\n",
    "    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())  # equivalent to cnopts.hostkeys=None\n",
    "\n",
    "    # ---- CONNECT ----\n",
    "    client.connect(\n",
    "        hostname=sftpHost,\n",
    "        port=sftpPort,\n",
    "        username=uname,\n",
    "        password=pwd,\n",
    "        allow_agent=False,\n",
    "        look_for_keys=False,\n",
    "    )\n",
    "\n",
    "    sftp = client.open_sftp()\n",
    "    print(\"Connected to SFTP Server!!!\")\n",
    "\n",
    "    # ---- DELETE LOCAL Vilbev FILES ----\n",
    "    for filename in os.listdir('.'):\n",
    "        if filename.startswith('Vilbev-') and filename.endswith('.zip'):\n",
    "            try:\n",
    "                os.remove(filename)\n",
    "                print(f'Deleted existing file: {filename}')\n",
    "            except Exception as e:\n",
    "                print(f'Error deleting {filename}: {e}')\n",
    "\n",
    "    # ---- REMOTE & LOCAL PATHS ----\n",
    "    remote_file = f\"/home/viljoenbev/Vilbev-{current_date}.zip\"\n",
    "    local_file = f\"./data/Vilbev-{current_date}.zip\"\n",
    "\n",
    "    # ---- DOWNLOAD ----\n",
    "    try:\n",
    "        sftp.get(\n",
    "            remotepath=remote_file,\n",
    "            localpath=local_file,\n",
    "            callback=None  # optionally add progress callback\n",
    "        )\n",
    "        print(f'Download is Complete!!! File saved as {local_file}')\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Remote file not found: {remote_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error downloading file: {e}\")\n",
    "\n",
    "    # ---- CLEAN UP ----\n",
    "    sftp.close()\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756f6369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Data:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to SFTP Server!!!\n",
      "Download is Complete!!! File saved as ./data/Vilbev-20260129.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.72s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1)):\n",
    "    download_data()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b8925e",
   "metadata": {},
   "source": [
    "## Unzip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c71c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract the first CSV file from a ZIP archive and load it into a pandas DataFrame.\n",
    "    Handles:\n",
    "    - file existence checks\n",
    "    - multiple CSV files (selects first match)\n",
    "    - safe extraction into a temp folder\n",
    "    - consistent return behavior\n",
    "    \"\"\"\n",
    "    zip_file_path = get_latest_zip(os.getenv('BASE_DIR'))\n",
    "\n",
    "    if not os.path.exists(zip_file_path):\n",
    "        raise FileNotFoundError(f\"‚ùå ZIP file does not exist: {zip_file_path}\")\n",
    "\n",
    "    print(f\"üì¶ Reading ZIP archive: {zip_file_path}\")\n",
    "\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "\n",
    "        # List all files\n",
    "        file_list = zip_ref.namelist()\n",
    "        print(\"üìÅ Files inside ZIP:\", file_list)\n",
    "\n",
    "        # find CSV file(s)\n",
    "        csv_files = [f for f in file_list if f.lower().endswith(\".csv\")]\n",
    "\n",
    "        if not csv_files:\n",
    "            raise ValueError(\"‚ùå No CSV file found inside ZIP.\")\n",
    "\n",
    "        # Use the first CSV file found\n",
    "        csv_file_name = csv_files[0]\n",
    "        print(f\"üìÑ Found CSV file: {csv_file_name}\")\n",
    "\n",
    "        # Ensure extraction directory exists\n",
    "        extract_dir = \"data\"\n",
    "        os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "        # Extract file (optional but useful for debugging)\n",
    "        extracted_path = zip_ref.extract(csv_file_name, path=extract_dir)\n",
    "        print(f\"üì§ Extracted to: {extracted_path}\")\n",
    "\n",
    "        # Load CSV into pandas directly from ZIP\n",
    "        with zip_ref.open(csv_file_name) as csv_file:\n",
    "            try:\n",
    "                df = pd.read_csv(csv_file)\n",
    "                print(f\"‚úÖ Loaded CSV: {csv_file_name}\")\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"‚ùå Failed to read CSV inside ZIP: {e}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51ba691",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = extract_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942b2977",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7fdc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to transform Viljoen Beverages data\n",
    "\n",
    "    Args:\n",
    "        df: Input dataframe to transform\n",
    "        returns: Transformed dataframe\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Standard column layout\n",
    "    columns=[\n",
    "        'SellerID','GUID','Date','Reference','Customer_Code','Name','Physical_Address1',\\\n",
    "        'Physical_Address2','Physical_Address3','Physical_Address4','Telephone',\\\n",
    "        'Stock_Code','Description','Price_Ex_Vat','Quantity','RepCode','ProductBarCodeID'\n",
    "        ]\n",
    "    # Create an empty dataframe\n",
    "    df1=pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Build the dataframe\n",
    "    df1['Date']=df['Date']\n",
    "    df1['SellerID']='VILJOEN'\n",
    "    df1['GUID']=0\n",
    "    df1['Reference']=df['Reference']\n",
    "    df1['Customer_Code']=df['Customer code']\n",
    "    df1['Name']=df['Customer name']\n",
    "    df1['Physical_Address1']=df['Physical_Address1']\n",
    "    df1['Physical_Address2']=df['Physical_Address2']\n",
    "    df1['Physical_Address3']=df['Physical_Address3']\n",
    "    df1['Physical_Address4']=(\n",
    "        df['Deliver1'].fillna('').astype(str) +' '+\n",
    "        df['Deliver2'].fillna('').astype(str) +' '+\n",
    "        df['Deliver3'].fillna('').astype(str) +' '+\n",
    "        df['Deliver4'].fillna('').astype(str)\n",
    "        ).str.strip()\n",
    "\n",
    "    df1['Telephone']=df['Telephone']\n",
    "    df1['Stock_Code']=df['Product code']\n",
    "    df1['Description']=df['Product description']\n",
    "    df1['Price_Ex_Vat']=round(abs(df['Value']/df['Quantity']),2)\n",
    "    df1['Quantity']=df['Quantity']\n",
    "    df1['RepCode']=df['Rep']\n",
    "    df1['ProductBarCodeID']=''\n",
    "\n",
    "    print(\"‚úÖ DATA TRANSFORMATION IN PROGRESS!!\")\n",
    "    print(f\"‚úÖ Total quantity: {np.sum(df1['Quantity']):.0f}\")\n",
    "\n",
    "    df2=df1.copy()\n",
    "    df2['Date']=pd.to_datetime(df2['Date'])\n",
    "    df2['Date']=df2['Date'].apply(lambda x: x.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "    #   INTELLIGENT NAME BACKFILLING\n",
    "    # -----------------------------------\n",
    "\n",
    "    print(\"üß† Handling missing buyer names intelligently..........\")\n",
    "    # 1) Use Customer_Code as primary matching key\n",
    "    # -----------------------------\n",
    "    df1['Name'] = df1.groupby('Customer_Code')['Name'].transform(\n",
    "        lambda x: x.fillna(x.mode().iloc[0]) if x.mode().size > 0 else x\n",
    "    )\n",
    "    # 2) Use Address fields as secondary matching key\n",
    "    # -----------------------------\n",
    "    df1['Name'] = df1.groupby(\n",
    "        ['Physical_Address1', 'Physical_Address2', 'Physical_Address3', 'Physical_Address4']\n",
    "    )['Name'].transform(\n",
    "        lambda x: x.fillna(x.mode().iloc[0]) if x.mode().size > 0 else x\n",
    "    )\n",
    "    # 3) Use telephone number as fallback\n",
    "    # -----------------------------\n",
    "    df1['Name'] = df1.groupby('Telephone')['Name'].transform(\n",
    "        lambda x: x.fillna(x.mode().iloc[0]) if x.mode().size > 0 else x\n",
    "    )\n",
    "    # 4) Global fallback (only for final unresolved missing names)\n",
    "    # -----------------------------\n",
    "    df1['Name'].fillna('SPAR NORTH RAND (11691)', inplace=True)\n",
    "    print(\"‚úÖ Missing buyer names fixed.....\")\n",
    "\n",
    "    #   DATE FORMAT CLEANING\n",
    "    # -----------------------------\n",
    "    print(\"‚úÖ Date fomat cleaned.........\")\n",
    "    df1['Date'] = pd.to_datetime(df1['Date'], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
    "    print(\"‚úÖ Data transformation complete!\")\n",
    "\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4022f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = transform_data(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e884766",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82a6483",
   "metadata": {},
   "source": [
    "## Validate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdb015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Function to validate data\n",
    "    \"\"\"\n",
    "    # logger = get_run_logger()\n",
    "    class Schema(pa.DataFrameModel):\n",
    "        # 1. Check data types and uniqueness\n",
    "        SellerID: Series[str] = pa.Field(nullable=False)  # seller IDs must be non-null\n",
    "        GUID: Series[int] = pa.Field(ge=0, nullable=False)  # must be non-null\n",
    "\n",
    "        # 2. Dates coerced to proper datetime\n",
    "        Date: Series[pd.Timestamp] = pa.Field(coerce=False, nullable=False) # must be non-null\n",
    "\n",
    "        # 3. Reference and customer codes\n",
    "        Reference: Series[str] = pa.Field(nullable=False) # must be non-null\n",
    "        Customer_Code: Series[str] = pa.Field(str_matches=r\"^[A-Z0-9]+$\", nullable=False)  # must be non-null\n",
    "\n",
    "        # 4. Customer details\n",
    "        Name: Series[str] = pa.Field(nullable=False) # must be non-null\n",
    "        Physical_Address1: Series[str] = pa.Field(nullable=True)\n",
    "        Physical_Address2: Series[str] = pa.Field(nullable=True)\n",
    "        Physical_Address3: Series[str] = pa.Field(nullable=True)\n",
    "        Physical_Address4: Series[str] = pa.Field(nullable=True)\n",
    "\n",
    "        # 5. Telephone validation (basic regex for digits, spaces, +, -)\n",
    "        Telephone: Series[str] = pa.Field(nullable=True)\n",
    "\n",
    "        # 6. Product details\n",
    "        Stock_Code: Series[str] = pa.Field(nullable=False) # must be non-null\n",
    "        Description: Series[str] = pa.Field(nullable=False) # must be non-null\n",
    "        Price_Ex_Vat: Series[float] = pa.Field(ge=0.0, nullable=False)  # must be non-null\n",
    "        Quantity: Series[int] = pa.Field(nullable=False)  # must be non-null\n",
    "\n",
    "        # 7. Rep and barcode\n",
    "        RepCode: Series[str] = pa.Field(nullable=True)\n",
    "        ProductBarCodeID: Series[str] = pa.Field(nullable=True)  # typical EAN/UPC\n",
    "\n",
    "        class Config:\n",
    "            strict = True  # enforce exact schema\n",
    "            coerce = True  # auto-convert types where possible\n",
    "\n",
    "    try:\n",
    "        # lazy=True means \"find all errors before crashing\"\n",
    "        Schema.validate(df, lazy=True)\n",
    "        print(\"‚úÖ Data passed validation! Proceeding to ETL...\")\n",
    "\n",
    "    except pa.errors.SchemaErrors as err:\n",
    "        print(\"‚ö†Ô∏è Data Contract Breached!.......\\n\")\n",
    "        print(f\"‚ùå Total errors found: {len(err.failure_cases)}\")\n",
    "\n",
    "        # Let's look at the specific failures\n",
    "        print(\"\\n*********‚ö†Ô∏èFailure Report‚ö†Ô∏è************\\n\")\n",
    "        print(err.failure_cases[['column', 'check', 'failure_case']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcac4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_data(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

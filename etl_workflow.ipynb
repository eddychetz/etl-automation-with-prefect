{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7b52809",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "8c5f6968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File System & Utilities\n",
    "# ----------------------\n",
    "import os\n",
    "import zipfile\n",
    "import getpass\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from utils import get_latest_zip # utils.py\n",
    "from typing import Tuple, Optional, Callable\n",
    "# import ftplib\n",
    "# import tempfile\n",
    "# from io import BytesIO\n",
    "\n",
    "# Data Manipulation & time\n",
    "# -------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "# FTP server communication\n",
    "# -------------------------\n",
    "import paramiko\n",
    "\n",
    "# Data Validation\n",
    "# ----------------\n",
    "import pandera as pa\n",
    "from pandera.typing.pandas import Series\n",
    "\n",
    "# For workflow automation\n",
    "# -----------------------\n",
    "from prefect import task, flow, get_run_logger # type: ignore\n",
    "\n",
    "# Ignore warnings\n",
    "# ----------------\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "ef4d84e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Eddie\\OneDrive - eRoute2Market\\eRoute2Market\\Agents\\etl-automation-with-prefect\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Print current working directory\n",
    "print(os.getcwd()) # os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5f6968",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "9cb4bd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data():\n",
    "    \"\"\"\n",
    "    Connects to SFTP and downloads Vilbev-{YYYYMMDD}.zip after removing\n",
    "    any existing Vilbev-*.zip files in ./data/raw.\n",
    "    \"\"\"\n",
    "    current_date = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "    # ---- PATHS ----\n",
    "    data_dir = Path(\"./data/raw\")\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    local_file = data_dir / f\"Vilbev-{current_date}.zip\"\n",
    "    remote_file = f\"/home/viljoenbev/Vilbev-{current_date}.zip\"\n",
    "\n",
    "    # ---- DELETE LOCAL Vilbev FILES FIRST ----\n",
    "    print(\"üßπ Cleaning up existing Vilbev-*.zip files in ./data/raw ...\")\n",
    "    deleted_any = False\n",
    "    for p in data_dir.glob(\"Vilbev-*.zip\"):\n",
    "        try:\n",
    "            p.unlink()\n",
    "            deleted_any = True\n",
    "            print(f\"üóëÔ∏è Deleted: {p.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error deleting {p.name}: {e}\")\n",
    "    if not deleted_any:\n",
    "        print(\"‚ÑπÔ∏è No existing Vilbev-*.zip files found to delete.\")\n",
    "\n",
    "    # (Optional) ensure target file does not exist‚Äîeven if name pattern changes in future\n",
    "    if local_file.exists():\n",
    "        try:\n",
    "            local_file.unlink()\n",
    "            print(f\"üóëÔ∏è Removed pre-existing target file: {local_file.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error deleting pre-existing target file {local_file.name}: {e}\")\n",
    "\n",
    "    # ---- PARAMIKO CLIENT SETUP ----\n",
    "    client = paramiko.SSHClient()\n",
    "    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "\n",
    "    sftp = None\n",
    "    try:\n",
    "        # ---- CONNECT ----\n",
    "        host = os.getenv('ftp_host')\n",
    "        port = int(os.getenv('ftp_port'))  # ensure integer\n",
    "        user = os.getenv('ftp_user')\n",
    "        pwd  = os.getenv('ftp_pass')\n",
    "\n",
    "        client.connect(\n",
    "            hostname=host,\n",
    "            port=port,\n",
    "            username=user,\n",
    "            password=pwd,\n",
    "            allow_agent=False,\n",
    "            look_for_keys=False,\n",
    "            timeout=30,\n",
    "        )\n",
    "        sftp = client.open_sftp()\n",
    "        print(\"üîê Connected to SFTP server\")\n",
    "\n",
    "        # ---- DOWNLOAD ----\n",
    "        print(f\"üì• Downloading: {remote_file} ‚Üí {local_file}\")\n",
    "        sftp.get(\n",
    "            remotepath=remote_file,\n",
    "            localpath=str(local_file),\n",
    "            callback=None  # add progress callback if you need it\n",
    "        )\n",
    "        print(\"‚úÖ Download complete!\")\n",
    "\n",
    "        print(f\"üìÅ File saved on {str(local_file)}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Remote file not found: {remote_file}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during SFTP operation: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # ---- CLEAN UP ----\n",
    "        try:\n",
    "            if sftp is not None:\n",
    "                sftp.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            client.close()\n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "756f6369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning up existing Vilbev-*.zip files in ./data/raw ...\n",
      "üóëÔ∏è Deleted: Vilbev-20260130.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîê Connected to SFTP server\n",
      "üì• Downloading: /home/viljoenbev/Vilbev-20260130.zip ‚Üí data\\raw\\Vilbev-20260130.zip\n",
      "‚úÖ Download complete!\n",
      "üìÅ File saved on data\\raw\\Vilbev-20260130.zip\n"
     ]
    }
   ],
   "source": [
    "download_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b8925e",
   "metadata": {},
   "source": [
    "## Unzip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "4c71c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract the first CSV file from a ZIP archive and load it into a pandas DataFrame.\n",
    "    Handles:\n",
    "    - file existence checks\n",
    "    - multiple CSV files (selects first match)\n",
    "    - safe extraction into a temp folder\n",
    "    - consistent return behavior\n",
    "    \"\"\"\n",
    "    zip_file_path = get_latest_zip(os.getenv('BASE_DIR'))\n",
    "\n",
    "    if not os.path.exists(zip_file_path):\n",
    "        raise FileNotFoundError(f\"‚ùå ZIP file does not exist: {zip_file_path}\")\n",
    "\n",
    "    print(\"üì¶ Reading ZIP archive!\")\n",
    "\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "\n",
    "        # List all files\n",
    "        file_list = zip_ref.namelist()\n",
    "        print(\"üìÅ Files inside ZIP:\", file_list)\n",
    "\n",
    "        # find CSV file(s)\n",
    "        csv_files = [f for f in file_list if f.lower().endswith(\".csv\")]\n",
    "\n",
    "        if not csv_files:\n",
    "            raise ValueError(\"‚ùå No CSV file found inside ZIP.\")\n",
    "\n",
    "        # Use the first CSV file found\n",
    "        csv_file_name = csv_files[0]\n",
    "        print(f\"üìÑ Found CSV file: {csv_file_name}\")\n",
    "\n",
    "        # Ensure extraction directory exists\n",
    "        extract_dir = \"data\"\n",
    "        os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "        # Extract file (optional but useful for debugging)\n",
    "        extracted_path = zip_ref.extract(csv_file_name, path=extract_dir)\n",
    "        print(f\"üì§ Extracted to: {extracted_path}\")\n",
    "\n",
    "        # Load CSV into pandas directly from ZIP\n",
    "        with zip_ref.open(csv_file_name) as csv_file:\n",
    "            try:\n",
    "                df = pd.read_csv(csv_file)\n",
    "                print(f\"‚úÖ Loaded CSV: {csv_file_name}\")\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"‚ùå Failed to read CSV inside ZIP: {e}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "a51ba691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Latest ZIP selected: C:\\Users\\Eddie\\OneDrive - eRoute2Market\\eRoute2Market\\Agents\\etl-automation-with-prefect\\data\\raw\\Vilbev-20260130.zip\n",
      "üì¶ Reading ZIP archive!\n",
      "üìÅ Files inside ZIP: ['Vilbev-20260130.csv']\n",
      "üìÑ Found CSV file: Vilbev-20260130.csv\n",
      "üì§ Extracted to: data\\Vilbev-20260130.csv\n",
      "‚úÖ Loaded CSV: Vilbev-20260130.csv\n"
     ]
    }
   ],
   "source": [
    "raw = extract_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942b2977",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "6c7fdc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to transform Viljoen Beverages data\n",
    "\n",
    "    Args:\n",
    "        df: Input dataframe to transform\n",
    "        returns: Transformed dataframe\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Standard column layout\n",
    "    columns=[\n",
    "        'SellerID','GUID','Date','Reference','Customer_Code','Name','Physical_Address1',\\\n",
    "        'Physical_Address2','Physical_Address3','Physical_Address4','Telephone',\\\n",
    "        'Stock_Code','Description','Price_Ex_Vat','Quantity','RepCode','ProductBarCodeID'\n",
    "        ]\n",
    "    # Create an empty dataframe\n",
    "    df1=pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Build the dataframe\n",
    "    df1['Date']=df['Date']\n",
    "    df1['SellerID']='VILJOEN'\n",
    "    df1['GUID']=0\n",
    "    df1['Reference']=df['Reference']\n",
    "    df1['Customer_Code']=df['Customer code']\n",
    "    df1['Name']=df['Customer name']\n",
    "    df1['Physical_Address1']=df['Physical_Address1']\n",
    "    df1['Physical_Address2']=df['Physical_Address2']\n",
    "    df1['Physical_Address3']=df['Physical_Address3']\n",
    "    df1['Physical_Address4']=(\n",
    "        df['Deliver1'].fillna('').astype(str) +' '+\n",
    "        df['Deliver2'].fillna('').astype(str) +' '+\n",
    "        df['Deliver3'].fillna('').astype(str) +' '+\n",
    "        df['Deliver4'].fillna('').astype(str)\n",
    "        ).str.strip()\n",
    "\n",
    "    df1['Telephone']=df['Telephone']\n",
    "    df1['Stock_Code']=df['Product code']\n",
    "    df1['Description']=df['Product description']\n",
    "    df1['Price_Ex_Vat']=round(abs(df['Value']/df['Quantity']),2)\n",
    "    df1['Quantity']=df['Quantity']\n",
    "    df1['RepCode']=df['Rep']\n",
    "    df1['ProductBarCodeID']=''\n",
    "\n",
    "    print(f\"‚ÑπÔ∏è Total quantity: {np.sum(df1['Quantity']):.0f}\\n\")\n",
    "\n",
    "    df2=df1.copy()\n",
    "    df2['Date']=pd.to_datetime(df2['Date'])\n",
    "    df2['Date']=df2['Date'].apply(lambda x: x.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "    #   INTELLIGENT NAME BACKFILLING\n",
    "    # -----------------------------------\n",
    "    # 1) Use Customer_Code as primary matching key\n",
    "    # -----------------------------\n",
    "    # df1['Name'] = df1.groupby('Customer_Code')['Name'].transform(\n",
    "    #     lambda x: x.fillna(x.mode().iloc[0]) if x.mode().size > 0 else x\n",
    "    # )\n",
    "    # # 2) Use Address fields as secondary matching key\n",
    "    # # -----------------------------\n",
    "    # df1['Name'] = df1.groupby(\n",
    "    #     ['Physical_Address1', 'Physical_Address2', 'Physical_Address3', 'Physical_Address4']\n",
    "    # )['Name'].transform(\n",
    "    #     lambda x: x.fillna(x.mode().iloc[0]) if x.mode().size > 0 else x\n",
    "    # )\n",
    "    # # 3) Use telephone number as fallback\n",
    "    # # -----------------------------\n",
    "    # df1['Name'] = df1.groupby('Telephone')['Name'].transform(\n",
    "    #     lambda x: x.fillna(x.mode().iloc[0]) if x.mode().size > 0 else x\n",
    "    # )\n",
    "    # # 4) Global fallback (only for final unresolved missing names)\n",
    "    # # -----------------------------\n",
    "    # df1['Name'].fillna('SPAR NORTH RAND (11691)', inplace=True)\n",
    "    # print(\"‚úÖ Missing buyer names fixed.\")\n",
    "\n",
    "    # Ensure you imported pandas as pd\n",
    "    # Condition: Physical_Address1 contains the phrase AND Name is missing/empty\n",
    "    mask = df1[\"Physical_Address1\"].astype(str).str.contains(\"SPAR NORTHRAND (11691)\", na=False)\n",
    "\n",
    "    df1[\"Name\"].fillna('SPAR NORTH RAND (11691)', inplace=True)\n",
    "    #   DATE FORMAT CLEANING\n",
    "    # -----------------------------\n",
    "    print(\"‚úÖ Date fomat cleaned\")\n",
    "    df1['Date'] = pd.to_datetime(df1['Date'], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
    "    print(\"‚úÖ Data transformation complete!\")\n",
    "\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "e4022f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è Total quantity: 1157\n",
      "\n",
      "‚úÖ Date fomat cleaned\n",
      "‚úÖ Data transformation complete!\n"
     ]
    }
   ],
   "source": [
    "df = transform_data(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "5faec767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "SellerID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "GUID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Reference",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Customer_Code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Physical_Address1",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Physical_Address2",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Physical_Address3",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Physical_Address4",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Telephone",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Stock_Code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Description",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Price_Ex_Vat",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Quantity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "RepCode",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ProductBarCodeID",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "42a7e919-9428-417d-a429-43a54e3dc833",
       "rows": [
        [
         "13",
         "VILJOEN",
         "0",
         "2026-01-29",
         "IN-431630",
         "SP067",
         "SPAR NORTH RAND (11691)",
         "SPAR NORTHRAND (11691)",
         "P.O.BOX 528",
         "CLAYVILLE",
         "SPAR EXPRESS AT MOOKETSI (31102) BUFFELSBERG CENTRE 1 MAIN ROAD LIMPOPO VAT: 4260101482",
         null,
         "SQ001",
         "Squeeze-Mee Chocolate 40x130ml",
         "298.67",
         "1.0",
         "18",
         ""
        ],
        [
         "14",
         "VILJOEN",
         "0",
         "2026-01-29",
         "IN-431630",
         "SP067",
         "SPAR NORTH RAND (11691)",
         "SPAR NORTHRAND (11691)",
         "P.O.BOX 528",
         "CLAYVILLE",
         "SPAR EXPRESS AT MOOKETSI (31102) BUFFELSBERG CENTRE 1 MAIN ROAD LIMPOPO VAT: 4260101482",
         null,
         "SQ002",
         "Squeeze-Mee Strawberry 40x130ml",
         "298.67",
         "1.0",
         "18",
         ""
        ],
        [
         "15",
         "VILJOEN",
         "0",
         "2026-01-29",
         "IN-431630",
         "SP067",
         "SPAR NORTH RAND (11691)",
         "SPAR NORTHRAND (11691)",
         "P.O.BOX 528",
         "CLAYVILLE",
         "SPAR EXPRESS AT MOOKETSI (31102) BUFFELSBERG CENTRE 1 MAIN ROAD LIMPOPO VAT: 4260101482",
         null,
         "SQ003",
         "Squeeze-Mee Bubblegum 40x130ml",
         "298.67",
         "1.0",
         "18",
         ""
        ],
        [
         "16",
         "VILJOEN",
         "0",
         "2026-01-29",
         "IN-431630",
         "SP067",
         "SPAR NORTH RAND (11691)",
         "SPAR NORTHRAND (11691)",
         "P.O.BOX 528",
         "CLAYVILLE",
         "SPAR EXPRESS AT MOOKETSI (31102) BUFFELSBERG CENTRE 1 MAIN ROAD LIMPOPO VAT: 4260101482",
         null,
         "SQ004",
         "Squeeze-Mee Vanilla 40x130ml",
         "298.67",
         "1.0",
         "18",
         ""
        ]
       ],
       "shape": {
        "columns": 17,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SellerID</th>\n",
       "      <th>GUID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Reference</th>\n",
       "      <th>Customer_Code</th>\n",
       "      <th>Name</th>\n",
       "      <th>Physical_Address1</th>\n",
       "      <th>Physical_Address2</th>\n",
       "      <th>Physical_Address3</th>\n",
       "      <th>Physical_Address4</th>\n",
       "      <th>Telephone</th>\n",
       "      <th>Stock_Code</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price_Ex_Vat</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>RepCode</th>\n",
       "      <th>ProductBarCodeID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>VILJOEN</td>\n",
       "      <td>0</td>\n",
       "      <td>2026-01-29</td>\n",
       "      <td>IN-431630</td>\n",
       "      <td>SP067</td>\n",
       "      <td>SPAR NORTH RAND (11691)</td>\n",
       "      <td>SPAR NORTHRAND (11691)</td>\n",
       "      <td>P.O.BOX 528</td>\n",
       "      <td>CLAYVILLE</td>\n",
       "      <td>SPAR EXPRESS AT MOOKETSI (31102) BUFFELSBERG C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SQ001</td>\n",
       "      <td>Squeeze-Mee Chocolate 40x130ml</td>\n",
       "      <td>298.67</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>VILJOEN</td>\n",
       "      <td>0</td>\n",
       "      <td>2026-01-29</td>\n",
       "      <td>IN-431630</td>\n",
       "      <td>SP067</td>\n",
       "      <td>SPAR NORTH RAND (11691)</td>\n",
       "      <td>SPAR NORTHRAND (11691)</td>\n",
       "      <td>P.O.BOX 528</td>\n",
       "      <td>CLAYVILLE</td>\n",
       "      <td>SPAR EXPRESS AT MOOKETSI (31102) BUFFELSBERG C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SQ002</td>\n",
       "      <td>Squeeze-Mee Strawberry 40x130ml</td>\n",
       "      <td>298.67</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>VILJOEN</td>\n",
       "      <td>0</td>\n",
       "      <td>2026-01-29</td>\n",
       "      <td>IN-431630</td>\n",
       "      <td>SP067</td>\n",
       "      <td>SPAR NORTH RAND (11691)</td>\n",
       "      <td>SPAR NORTHRAND (11691)</td>\n",
       "      <td>P.O.BOX 528</td>\n",
       "      <td>CLAYVILLE</td>\n",
       "      <td>SPAR EXPRESS AT MOOKETSI (31102) BUFFELSBERG C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SQ003</td>\n",
       "      <td>Squeeze-Mee Bubblegum 40x130ml</td>\n",
       "      <td>298.67</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>VILJOEN</td>\n",
       "      <td>0</td>\n",
       "      <td>2026-01-29</td>\n",
       "      <td>IN-431630</td>\n",
       "      <td>SP067</td>\n",
       "      <td>SPAR NORTH RAND (11691)</td>\n",
       "      <td>SPAR NORTHRAND (11691)</td>\n",
       "      <td>P.O.BOX 528</td>\n",
       "      <td>CLAYVILLE</td>\n",
       "      <td>SPAR EXPRESS AT MOOKETSI (31102) BUFFELSBERG C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SQ004</td>\n",
       "      <td>Squeeze-Mee Vanilla 40x130ml</td>\n",
       "      <td>298.67</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SellerID  GUID        Date  Reference Customer_Code  \\\n",
       "13  VILJOEN     0  2026-01-29  IN-431630         SP067   \n",
       "14  VILJOEN     0  2026-01-29  IN-431630         SP067   \n",
       "15  VILJOEN     0  2026-01-29  IN-431630         SP067   \n",
       "16  VILJOEN     0  2026-01-29  IN-431630         SP067   \n",
       "\n",
       "                       Name       Physical_Address1 Physical_Address2  \\\n",
       "13  SPAR NORTH RAND (11691)  SPAR NORTHRAND (11691)       P.O.BOX 528   \n",
       "14  SPAR NORTH RAND (11691)  SPAR NORTHRAND (11691)       P.O.BOX 528   \n",
       "15  SPAR NORTH RAND (11691)  SPAR NORTHRAND (11691)       P.O.BOX 528   \n",
       "16  SPAR NORTH RAND (11691)  SPAR NORTHRAND (11691)       P.O.BOX 528   \n",
       "\n",
       "   Physical_Address3                                  Physical_Address4  \\\n",
       "13         CLAYVILLE  SPAR EXPRESS AT MOOKETSI (31102) BUFFELSBERG C...   \n",
       "14         CLAYVILLE  SPAR EXPRESS AT MOOKETSI (31102) BUFFELSBERG C...   \n",
       "15         CLAYVILLE  SPAR EXPRESS AT MOOKETSI (31102) BUFFELSBERG C...   \n",
       "16         CLAYVILLE  SPAR EXPRESS AT MOOKETSI (31102) BUFFELSBERG C...   \n",
       "\n",
       "   Telephone Stock_Code                      Description  Price_Ex_Vat  \\\n",
       "13       NaN      SQ001   Squeeze-Mee Chocolate 40x130ml        298.67   \n",
       "14       NaN      SQ002  Squeeze-Mee Strawberry 40x130ml        298.67   \n",
       "15       NaN      SQ003   Squeeze-Mee Bubblegum 40x130ml        298.67   \n",
       "16       NaN      SQ004     Squeeze-Mee Vanilla 40x130ml        298.67   \n",
       "\n",
       "    Quantity  RepCode ProductBarCodeID  \n",
       "13       1.0       18                   \n",
       "14       1.0       18                   \n",
       "15       1.0       18                   \n",
       "16       1.0       18                   "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[13:17]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82a6483",
   "metadata": {},
   "source": [
    "## Validate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "bcdb015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Function to validate data\n",
    "    \"\"\"\n",
    "    # logger = get_run_logger()\n",
    "    class Schema(pa.DataFrameModel):\n",
    "        # 1. Check data types and uniqueness\n",
    "        SellerID: Series[str] = pa.Field(nullable=False)  # seller IDs must be non-null\n",
    "        GUID: Series[int] = pa.Field(ge=0, nullable=False)  # must be non-null\n",
    "\n",
    "        # 2. Dates coerced to proper datetime\n",
    "        Date: Series[pd.Timestamp] = pa.Field(coerce=False, nullable=False) # must be non-null\n",
    "\n",
    "        # 3. Reference and customer codes\n",
    "        Reference: Series[str] = pa.Field(nullable=False) # must be non-null\n",
    "        Customer_Code: Series[str] = pa.Field(str_matches=r\"^[A-Z0-9]+$\", nullable=False)  # must be non-null\n",
    "\n",
    "        # 4. Customer details\n",
    "        Name: Series[str] = pa.Field(nullable=False) # must be non-null\n",
    "        Physical_Address1: Series[str] = pa.Field(nullable=True)\n",
    "        Physical_Address2: Series[str] = pa.Field(nullable=True)\n",
    "        Physical_Address3: Series[str] = pa.Field(nullable=True)\n",
    "        Physical_Address4: Series[str] = pa.Field(nullable=True)\n",
    "\n",
    "        # 5. Telephone validation (basic regex for digits, spaces, +, -)\n",
    "        Telephone: Series[str] = pa.Field(nullable=True)\n",
    "\n",
    "        # 6. Product details\n",
    "        Stock_Code: Series[str] = pa.Field(nullable=False) # must be non-null\n",
    "        Description: Series[str] = pa.Field(nullable=False) # must be non-null\n",
    "        Price_Ex_Vat: Series[float] = pa.Field(ge=0.0, nullable=False)  # must be non-null\n",
    "        Quantity: Series[int] = pa.Field(nullable=False)  # must be non-null\n",
    "\n",
    "        # 7. Rep and barcode\n",
    "        RepCode: Series[str] = pa.Field(nullable=True)\n",
    "        ProductBarCodeID: Series[str] = pa.Field(nullable=True)  # typical EAN/UPC\n",
    "\n",
    "        class Config:\n",
    "            strict = True  # enforce exact schema\n",
    "            coerce = True  # auto-convert types where possible\n",
    "\n",
    "    try:\n",
    "        # lazy=True means \"find all errors before crashing\"\n",
    "        Schema.validate(df, lazy=True)\n",
    "        print(\"‚úÖ Data passed validation!\\n‚ÑπÔ∏è Proceeding to next step.\")\n",
    "\n",
    "    except pa.errors.SchemaErrors as err:\n",
    "        print(\"‚ö†Ô∏è Data Contract Breached!.......\\n\")\n",
    "        print(f\"‚ùå Total errors found: {len(err.failure_cases)}\")\n",
    "\n",
    "        # Let's look at the specific failures\n",
    "        print(\"\\n*********‚ö†Ô∏èFailure Report‚ö†Ô∏è************\\n\")\n",
    "        print(err.failure_cases[['column', 'check', 'failure_case']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "1bcac4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data passed validation!\n",
      "‚ÑπÔ∏è Proceeding to next step.\n"
     ]
    }
   ],
   "source": [
    "validate_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e5fb35",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "a31130b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Validate date in cleaned file\n",
    "# -------------------------------------------------------------\n",
    "def validate_dates(\n",
    "    min_date: pd.Timestamp,\n",
    "    max_date: pd.Timestamp,\n",
    "    today: datetime = None,\n",
    "    lookback_days: int = 3) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Raises ValueError if the date range is not entirely within the last `lookback_days`\n",
    "    and if the latest month is neither the current month nor the previous month.\n",
    "    \"\"\"\n",
    "\n",
    "    if today is None:\n",
    "        today = datetime.now()\n",
    "\n",
    "    # Normalize to date (drop time)\n",
    "    today_d = today.date()\n",
    "    window_start = today_d - timedelta(days=lookback_days)\n",
    "\n",
    "    min_d = min_date.date()\n",
    "    max_d = max_date.date()\n",
    "\n",
    "    # 1) Entire range must be within the last `lookback_days` days (inclusive)\n",
    "    if not (window_start <= min_d <= today_d and window_start <= max_d <= today_d):\n",
    "        raise ValueError(\n",
    "            f\"‚ùå Date range {min_d} to {max_d} is not fully within the last {lookback_days} days \"\n",
    "            f\"({window_start}..{today_d}).\"\n",
    "        )\n",
    "\n",
    "    # 2) Month check on the latest date in the file (max_d)\n",
    "    cur_month = today_d.month\n",
    "    prev_month = 12 if cur_month == 1 else cur_month - 1\n",
    "    file_month = max_d.month\n",
    "\n",
    "    if file_month not in (cur_month, prev_month):\n",
    "        raise ValueError(\n",
    "            f\"‚ùå Latest file month ({file_month}) is not the current month ({cur_month}) \"\n",
    "            f\"or previous month ({prev_month}).\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_local(\n",
    "    df: pd.DataFrame,\n",
    "    create_dir_if_missing: bool = True,\n",
    "    delete_existing_csvs: bool = True,        # ‚Üê new flag to control cleanup\n",
    "    restrict_delete_to_prefix: str | None = None  # e.g., \"Viljoenbev_\" to only delete those CSVs\n",
    ") -> Tuple[str, bool]:\n",
    "    \"\"\"\n",
    "    Save cleaned data to a CSV inside the folder specified by OUTPUT_DIR in .env,\n",
    "    only if:\n",
    "    - the DataFrame's date range is entirely within the last 3 days, and\n",
    "    - the latest date's month is the current month or the previous month.\n",
    "    Skips save if a file with the same name already exists.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (full_path, saved) : Tuple[str, bool]\n",
    "        full_path -> absolute path to the intended CSV\n",
    "        saved     -> True if file was written, False if skipped (already existed)\n",
    "    \"\"\"\n",
    "    # --- Resolve OUTPUT_DIR ---\n",
    "    output_dir = os.getenv(\"OUTPUT_DIR\")\n",
    "    if not output_dir:\n",
    "        raise ValueError(\"Environment variable 'OUTPUT_DIR' is not set in your environment or .env file.\")\n",
    "\n",
    "    output_dir_path = Path(os.path.abspath(os.path.expanduser(output_dir)))\n",
    "    if not output_dir_path.is_dir():\n",
    "        if create_dir_if_missing:\n",
    "            output_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "            print(f\"üìÅ Created output directory: {output_dir_path}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Output directory does not exist: {output_dir_path}\")\n",
    "\n",
    "    # ---- DELETE EXISTING CSVs IN CLEANED FOLDER (before saving) ----\n",
    "    if delete_existing_csvs:\n",
    "        # Choose the pattern:\n",
    "        #   \"*.csv\" to delete ALL CSVs in the folder\n",
    "        #   f\"{restrict_delete_to_prefix}*.csv\" to only delete those starting with a prefix\n",
    "        if restrict_delete_to_prefix:\n",
    "            pattern = f\"{restrict_delete_to_prefix}*.csv\"\n",
    "        else:\n",
    "            pattern = \"*.csv\"\n",
    "\n",
    "        print(f\"üßπ Cleaning up existing CSV file in:\\nüìÅ {output_dir_path}.\")\n",
    "        deleted_any = False\n",
    "        for p in output_dir_path.glob(pattern):\n",
    "            try:\n",
    "                p.unlink()\n",
    "                deleted_any = True\n",
    "                print(f\"üóëÔ∏è Deleted CSV: {p.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error deleting {p.name}: {e}\")\n",
    "        if not deleted_any:\n",
    "            print(\"‚ÑπÔ∏è No matching CSV files found to delete.\")\n",
    "\n",
    "    # --- Prepare and validate dates ---\n",
    "    if \"Date\" not in df.columns:\n",
    "        raise KeyError(\"Input DataFrame must contain a 'Date' column.\")\n",
    "\n",
    "    data = df.copy()\n",
    "    data[\"Date\"] = pd.to_datetime(data[\"Date\"], errors=\"coerce\")\n",
    "\n",
    "    if data[\"Date\"].isna().all():\n",
    "        raise ValueError(\"All values in 'Date' are NaT after parsing. Check your input data.\")\n",
    "\n",
    "    min_date = data[\"Date\"].dropna().min()\n",
    "    max_date = data[\"Date\"].dropna().max()\n",
    "\n",
    "    # Validation per your rule:\n",
    "    validate_dates(min_date, max_date, lookback_days=3)\n",
    "\n",
    "    # --- Build deterministic filename and check for existence ---\n",
    "    min_str = min_date.strftime(\"%Y-%m-%d\")\n",
    "    max_str = max_date.strftime(\"%Y-%m-%d\")\n",
    "    filename = f\"Viljoenbev_{min_str}_to_{max_str}.csv\"\n",
    "    full_path = output_dir_path / filename\n",
    "\n",
    "    if full_path.exists():\n",
    "        print(f\"üõë File already exists, skipping save:\\nüìÅ {full_path}\")\n",
    "        return str(full_path), False\n",
    "\n",
    "    # --- Finalize and save ---\n",
    "    data[\"Date\"] = data[\"Date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "    data.to_csv(full_path, index=False)\n",
    "    print(f\"\\n‚úÖ Data saved to:\\nüìÅ {full_path}\")\n",
    "    return str(full_path), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "66c94f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning up existing CSV file in:\n",
      "üìÅ c:\\Users\\Eddie\\OneDrive - eRoute2Market\\eRoute2Market\\Agents\\etl-automation-with-prefect\\data\\cleaned.\n",
      "üóëÔ∏è Deleted CSV: Viljoenbev_2026-01-29_to_2026-01-29.csv\n",
      "\n",
      "‚úÖ Data saved to:\n",
      "üìÅ c:\\Users\\Eddie\\OneDrive - eRoute2Market\\eRoute2Market\\Agents\\etl-automation-with-prefect\\data\\cleaned\\Viljoenbev_2026-01-29_to_2026-01-29.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('c:\\\\Users\\\\Eddie\\\\OneDrive - eRoute2Market\\\\eRoute2Market\\\\Agents\\\\etl-automation-with-prefect\\\\data\\\\cleaned\\\\Viljoenbev_2026-01-29_to_2026-01-29.csv',\n",
       " True)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_to_local(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9132f0",
   "metadata": {},
   "source": [
    "## Upload to FTP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "0e1d3052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_server(csv_file_path: str = None,) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Upload a specific CSV file to an SFTP server using only paramiko.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    csv_file_path : str\n",
    "        Path to the local CSV file to upload. If None, it will pick the first\n",
    "        file matching 'Viljoenbev_*.csv' in `output_dir`.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str or None\n",
    "        Remote path where the file was uploaded, or None if the upload failed.\n",
    "    \"\"\"\n",
    "    # ---- CONNECT ----\n",
    "    sftp_host = os.getenv('ftp_host')\n",
    "    sftp_port = int(os.getenv('ftp_port'))  # ensure integer\n",
    "    sftp_user = os.getenv('ftp_user')\n",
    "    sftp_pass  = os.getenv('ftp_pass')\n",
    "\n",
    "    remote_dir = os.getenv('ftp_dir')\n",
    "\n",
    "    # Fallback to discover a file if not provided (mirrors your original default idea)\n",
    "    if csv_file_path is None:\n",
    "        # Adjust `output_dir` to your actual variable/scope if needed\n",
    "        output_dir = os.getenv('OUTPUT_DIR')\n",
    "        matches = glob(os.path.join(output_dir, 'Viljoenbev_*.csv'))\n",
    "        if not matches:\n",
    "            print(\"‚ö†Ô∏è No CSV file found matching 'Viljoenbev_*.csv'.\")\n",
    "            return None\n",
    "        csv_file_path = matches[0]\n",
    "\n",
    "    try:\n",
    "        # Verify the local file exists\n",
    "        if not os.path.exists(csv_file_path):\n",
    "            print(f\"‚ö†Ô∏è Local file not found: {csv_file_path}\")\n",
    "            return None\n",
    "\n",
    "        filename = os.path.basename(csv_file_path)\n",
    "        # Normalize remote path to POSIX style for SFTP\n",
    "        remote_dir_posix = remote_dir.replace('\\\\', '/').rstrip('/') + '/'\n",
    "        remote_path = (remote_dir_posix + filename)\n",
    "\n",
    "        # Create SSH client and connect\n",
    "        ssh = paramiko.SSHClient()\n",
    "\n",
    "        # WARNING: Auto-adding host keys reduces security. Prefer loading known hosts in production.\n",
    "        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "\n",
    "        print(f\"üîê Connecting to FTP server as {sftp_user}.\")\n",
    "        ssh.connect(\n",
    "            hostname=sftp_host,\n",
    "            port=sftp_port,\n",
    "            username=sftp_user,\n",
    "            password=sftp_pass,\n",
    "            look_for_keys=False,\n",
    "            allow_agent=False,\n",
    "            timeout=30\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            # Open SFTP session\n",
    "            sftp = ssh.open_sftp()\n",
    "\n",
    "            # Ensure remote directory exists (create recursively if missing)\n",
    "            _ensure_remote_dir(sftp, remote_dir_posix)\n",
    "\n",
    "            # Upload with confirmation via file size/stat check\n",
    "            print(f\"üöÄ Uploading {filename} to {remote_path}...\")\n",
    "            sftp.put(csv_file_path, remote_path)\n",
    "\n",
    "            # Optional: verify upload completed by checking remote file size\n",
    "            local_size = os.path.getsize(csv_file_path)\n",
    "            remote_stat = sftp.stat(remote_path)\n",
    "            if remote_stat.st_size != local_size:\n",
    "                print(\"‚ö†Ô∏è Size mismatch after upload. Upload may be incomplete.\")\n",
    "                return None\n",
    "\n",
    "            print(\"‚úÖ Upload completed successfully!\")\n",
    "            return None\n",
    "\n",
    "        finally:\n",
    "            try:\n",
    "                sftp.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            ssh.close()\n",
    "\n",
    "    except paramiko.AuthenticationException:\n",
    "        print(\"‚ö†Ô∏è Authentication failed. Please verify username/password (or key).\")\n",
    "        return None\n",
    "    except paramiko.SSHException as e:\n",
    "        print(f\"‚ùå SSH/SFTP error: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error uploading file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Remote Directory Helpers\n",
    "# ------------------------------\n",
    "def _ensure_remote_dir(sftp: paramiko.SFTPClient, remote_dir_posix: str) -> None:\n",
    "    \"\"\"\n",
    "    Recursively create remote directories if they do not exist.\n",
    "    `remote_dir_posix` must be a POSIX-style path ending with '/'.\n",
    "    \"\"\"\n",
    "    # Split path into components and build progressively\n",
    "    # Handle absolute paths like '/home/user/data/'\n",
    "    parts = [p for p in remote_dir_posix.split('/') if p]\n",
    "    prefix = '/' if remote_dir_posix.startswith('/') else ''\n",
    "\n",
    "    current = prefix\n",
    "    for part in parts:\n",
    "        current = (current.rstrip('/') + '/' + part)\n",
    "        try:\n",
    "            sftp.stat(current)  # Exists\n",
    "        except FileNotFoundError:\n",
    "            sftp.mkdir(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "3c449549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîê Connecting to FTP server as viljoenbev.\n",
      "üöÄ Uploading Viljoenbev_2026-01-29_to_2026-01-29.csv to /home/viljoenbev/data/Viljoenbev_2026-01-29_to_2026-01-29.csv...\n",
      "‚úÖ Upload completed successfully!\n"
     ]
    }
   ],
   "source": [
    "upload_to_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edb18fe",
   "metadata": {},
   "source": [
    "## Run import Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a214df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_import(timeout: int = 120):\n",
    "    \"\"\"\n",
    "    Connect to a remote server via SSH and execute commands, first testing then running.\n",
    "\n",
    "    Args:\n",
    "        timeout (int): Timeout for command execution in seconds\n",
    "\n",
    "    Returns:\n",
    "        tuple: (stdout, stderr, exit_code) from the last command executed\n",
    "    \"\"\"\n",
    "    # Access login credentials\n",
    "    hostname = os.getenv('host_name')\n",
    "    port = int(os.getenv('port'))\n",
    "    username = os.getenv('user_name')\n",
    "    password = os.getenv('password')\n",
    "\n",
    "    # Define commands\n",
    "    commands = {\n",
    "        'test': \"/usr/local/eroute2market/supply_chain/scripts/importtxns.pl /home/viljoenbev/data 1\",\n",
    "        'run': \"/usr/local/eroute2market/supply_chain/scripts/importtxns.pl /home/viljoenbev/data 1\"\n",
    "    }\n",
    "\n",
    "    # If no password provided, prompt for it securely\n",
    "    if password is None:\n",
    "        password = getpass.getpass(f\"Enter SSH password for {username}@{hostname}: \")\n",
    "\n",
    "    # Create an SSH client instance\n",
    "    client = paramiko.SSHClient()\n",
    "\n",
    "    try:\n",
    "        # Automatically add the server's host key\n",
    "        client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "\n",
    "        # Connect to the remote server\n",
    "        print(f\"Connecting to {hostname} on port {port}...\")\n",
    "        client.connect(\n",
    "            hostname=hostname,\n",
    "            port=port,\n",
    "            username=username,\n",
    "            password=password\n",
    "        )\n",
    "\n",
    "        # First execute the test command\n",
    "        print(f\"Executing test command:>>>>>>>> {commands['test']}\")\n",
    "        stdin, stdout, stderr = client.exec_command(commands['test'], timeout=timeout)\n",
    "        exit_status = stdout.channel.recv_exit_status()\n",
    "        stdout_str = stdout.read().decode('utf-8')\n",
    "        stderr_str = stderr.read().decode('utf-8')\n",
    "\n",
    "        if exit_status != 0:\n",
    "            print(f\"Test command failed with exit code: {exit_status}\")\n",
    "            print(\"Aborting - not running the main command.\")\n",
    "            return stdout_str, stderr_str, exit_status\n",
    "\n",
    "        print(\"Test command succeeded. \\nNow executing run command...\")\n",
    "\n",
    "        # If test succeeded, execute the run command\n",
    "        print(f\"Executing run command:>>>>>>>> {commands['run']}\")\n",
    "        stdin, stdout, stderr = client.exec_command(commands['run'], timeout=timeout)\n",
    "        exit_status = stdout.channel.recv_exit_status()\n",
    "        stdout_str = stdout.read().decode('utf-8')\n",
    "        stderr_str = stderr.read().decode('utf-8')\n",
    "\n",
    "        # Print status\n",
    "        if exit_status == 0:\n",
    "            print(\"Run command executed successfully.\")\n",
    "        else:\n",
    "            print(f\"Run command failed with exit code: {exit_status}\")\n",
    "\n",
    "        # Return results from the run command\n",
    "        return stdout_str, stderr_str, exit_status\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return \"\", str(e), -1\n",
    "\n",
    "    finally:\n",
    "        # Always close the connection\n",
    "        client.close()\n",
    "        print(\"SSH connection closed.\")\n",
    "\n",
    "# Helper Function\n",
    "# -----------------\n",
    "def parse_perl_output(stdout: str, stderr: str, exit_code: int) -> dict:\n",
    "    working_messages = [line for line in stdout.splitlines() if line.strip()]\n",
    "\n",
    "    stderr_lines = [line for line in stderr.splitlines() if line.strip()]\n",
    "    warnings = [warn for warn in stderr_lines if \"warning\" in warn.lower()]\n",
    "    errors = [err for err in stderr_lines if \"error\" in err.lower()]\n",
    "    other_stderr = [serr for serr in stderr_lines if serr not in warnings + errors]\n",
    "\n",
    "    return {\n",
    "        \"working_on\": working_messages,\n",
    "        \"warnings\": warnings,\n",
    "        \"errors\": errors,\n",
    "        \"other_stderr\": other_stderr,\n",
    "        \"exit_code\": exit_code\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "32f41713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to eroute2market.co.za on port 28...\n",
      "Executing test command:>>>>>>>> /usr/local/eroute2market/supply_chain/scripts/importtxns.pl /home/viljoenbev/data 1\n",
      "Test command succeeded. \n",
      "Now executing run command...\n",
      "Executing run command:>>>>>>>> /usr/local/eroute2market/supply_chain/scripts/importtxns.pl /home/viljoenbev/data\n",
      "Run command executed successfully.\n",
      "SSH connection closed.\n",
      "working_on: ['Working on /home/viljoenbev/data/Viljoenbev_2026-01-29_to_2026-01-29.csv', 'Date from 2026-01-29 to 2026-01-29', 'Renaming ...']\n",
      "warnings: ['WARNING: MYSQL_OPT_RECONNECT is deprecated and will be removed in a future version.', 'WARNING: MYSQL_OPT_RECONNECT is deprecated and will be removed in a future version.']\n",
      "errors: []\n",
      "other_stderr: []\n",
      "exit_code: 0\n"
     ]
    }
   ],
   "source": [
    "out, err, cod = run_import()\n",
    "result = parse_perl_output(out, err, cod)\n",
    "for key, value in result.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025bd8b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e85934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "97f0c130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ALERT] Authentication failed. Check username/password or switch to key-based auth.\n",
      "Exit: -1\n",
      "STDOUT:\n",
      " \n",
      "STDERR:\n",
      " Authentication failed. Check username/password or switch to key-based auth.\n"
     ]
    }
   ],
   "source": [
    "# Simple console alerts (default)\n",
    "stdout, stderr, code = run_import_commands()\n",
    "print(\"Exit:\", code)\n",
    "print(\"STDOUT:\\n\", stdout)\n",
    "print(\"STDERR:\\n\", stderr)\n",
    "\n",
    "# Or send alerts to Microsoft Teams via an incoming webhook:\n",
    "# import json, urllib.request\n",
    "# def teams_notifier(body: str):\n",
    "#     url = \"https://outlook.office.com/webhook/...\"  # your Teams incoming webhook\n",
    "#     req = urllib.request.Request(\n",
    "#         url,\n",
    "#         data=json.dumps({\"text\": body}).encode(\"utf-8\"),\n",
    "#         headers={\"Content-Type\": \"application/json\"},\n",
    "#         method=\"POST\"\n",
    "#     )\n",
    "#     with urllib.request.urlopen(req) as resp: _ = resp.read()\n",
    "#\n",
    "# stdout, stderr, code = run_import_commands(\n",
    "#     username=\"toby\",\n",
    "#     password=\"your_password_here\",\n",
    "#     notifier=teams_notifier\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

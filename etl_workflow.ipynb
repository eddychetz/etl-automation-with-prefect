{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7b52809",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45261c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5f6968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "os.getcwd()\n",
    "import ftplib\n",
    "import tempfile\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "import paramiko\n",
    "import pandera as pa\n",
    "from pandera.typing.pandas import Index, DataFrame, Series\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from datetime import date, timedelta, datetime\n",
    "import time\n",
    "from typing import Tuple, Dict, List, Optional, Iterable, Callable, TypeVar, Any\n",
    "\n",
    "from prefect import task, flow, get_run_logger # type: ignore\n",
    "\n",
    "from utils import get_latest_zip\n",
    "from glob import glob\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202b24d8",
   "metadata": {},
   "source": [
    "## Progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0906b945",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3062625025.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[226]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mjupyter nbextension enable --py widgetsnbextension\u001b[39m\n            ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%pip install ipywidgets jupyterlab_widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba62c4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipywidgets'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[221]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mipywidgets\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mipywidgets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IntProgress\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[32m10\u001b[39m), desc=\u001b[33m\"\u001b[39m\u001b[33mDownloading data\u001b[39m\u001b[33m\"\u001b[39m, bar_format=\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{desc}\u001b[39;00m\u001b[33m: | \u001b[39m\u001b[38;5;132;01m{percentage:3.0f}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'ipywidgets'"
     ]
    }
   ],
   "source": [
    "import ipywidgets\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(10), desc=\"Downloading data\", bar_format=\"{desc}: | {percentage:3.0f}%\"):\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc18a6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(*, progress=None) -> List[str]:\n",
    "    sftpHost = os.getenv('ftp_host')\n",
    "    sftpPort = int(os.getenv('ftp_port'))\n",
    "    uname = os.getenv('ftp_user')\n",
    "    pwd = os.getenv('ftp_pass')\n",
    "\n",
    "    current_date = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "    # ---- PARAMIKO CLIENT SETUP (replaces pysftp.CnOpts) ----\n",
    "    client = paramiko.SSHClient()\n",
    "    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())  # equivalent to cnopts.hostkeys=None\n",
    "\n",
    "    # ---- CONNECT ----\n",
    "    client.connect(\n",
    "        hostname=sftpHost,\n",
    "        port=sftpPort,\n",
    "        username=uname,\n",
    "        password=pwd,\n",
    "        allow_agent=False,\n",
    "        look_for_keys=False,\n",
    "    )\n",
    "\n",
    "    sftp = client.open_sftp()\n",
    "    print(\"Connected to SFTP Server!!!\")\n",
    "\n",
    "    # ---- DELETE LOCAL Vilbev FILES ----\n",
    "    for filename in os.listdir('.'):\n",
    "        if filename.startswith('Vilbev-') and filename.endswith('.zip'):\n",
    "            try:\n",
    "                os.remove(filename)\n",
    "                print(f'Deleted existing file: {filename}')\n",
    "            except Exception as e:\n",
    "                print(f'Error deleting {filename}: {e}')\n",
    "\n",
    "    # ---- REMOTE & LOCAL PATHS ----\n",
    "    remote_file = f\"/home/viljoenbev/Vilbev-{current_date}.zip\"\n",
    "    local_file = f\"./data/Vilbev-{current_date}.zip\"\n",
    "\n",
    "    # ---- DOWNLOAD ----\n",
    "    try:\n",
    "        sftp.get(\n",
    "            remotepath=remote_file,\n",
    "            localpath=local_file,\n",
    "            callback=None  # optionally add progress callback\n",
    "        )\n",
    "        print(f'Download is Complete!!! File saved as {local_file}')\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Remote file not found: {remote_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error downloading file: {e}\")\n",
    "\n",
    "    # ---- CLEAN UP ----\n",
    "    sftp.close()\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756f6369",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[227]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDownloading data\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbar_format\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{desc}\u001b[39;49;00m\u001b[33;43m: | \u001b[39;49m\u001b[38;5;132;43;01m{percentage:3.0f}\u001b[39;49;00m\u001b[33;43m%\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[32m      2\u001b[39m     download_data()\n\u001b[32m      3\u001b[39m     time.sleep(\u001b[32m0.0001\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Eddie\\OneDrive - eRoute2Market\\eRoute2Market\\Agents\\etl-automation-with-prefect\\.venv\\Lib\\site-packages\\tqdm\\notebook.py:234\u001b[39m, in \u001b[36mtqdm_notebook.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    232\u001b[39m unit_scale = \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.unit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.unit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m    233\u001b[39m total = \u001b[38;5;28mself\u001b[39m.total * unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.total \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.total\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m \u001b[38;5;28mself\u001b[39m.container = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[38;5;28mself\u001b[39m.container.pbar = proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    236\u001b[39m \u001b[38;5;28mself\u001b[39m.displayed = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Eddie\\OneDrive - eRoute2Market\\eRoute2Market\\Agents\\etl-automation-with-prefect\\.venv\\Lib\\site-packages\\tqdm\\notebook.py:108\u001b[39m, in \u001b[36mtqdm_notebook.status_printer\u001b[39m\u001b[34m(_, total, desc, ncols)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[32m    110\u001b[39m     pbar = IProgress(\u001b[38;5;28mmin\u001b[39m=\u001b[32m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m=total)\n",
      "\u001b[31mImportError\u001b[39m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Data:   0%|          | 0/10 [36:59<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for row in tqdm(range(10)):\n",
    "    download_data()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b8925e",
   "metadata": {},
   "source": [
    "## Unzip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c71c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract the first CSV file from a ZIP archive and load it into a pandas DataFrame.\n",
    "    Handles:\n",
    "    - file existence checks\n",
    "    - multiple CSV files (selects first match)\n",
    "    - safe extraction into a temp folder\n",
    "    - consistent return behavior\n",
    "    \"\"\"\n",
    "    zip_file_path = get_latest_zip(os.getenv('BASE_DIR'))\n",
    "\n",
    "    if not os.path.exists(zip_file_path):\n",
    "        raise FileNotFoundError(f\"‚ùå ZIP file does not exist: {zip_file_path}\")\n",
    "\n",
    "    print(f\"üì¶ Reading ZIP archive: {zip_file_path}\")\n",
    "\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "\n",
    "        # List all files\n",
    "        file_list = zip_ref.namelist()\n",
    "        print(\"üìÅ Files inside ZIP:\", file_list)\n",
    "\n",
    "        # find CSV file(s)\n",
    "        csv_files = [f for f in file_list if f.lower().endswith(\".csv\")]\n",
    "\n",
    "        if not csv_files:\n",
    "            raise ValueError(\"‚ùå No CSV file found inside ZIP.\")\n",
    "\n",
    "        # Use the first CSV file found\n",
    "        csv_file_name = csv_files[0]\n",
    "        print(f\"üìÑ Found CSV file: {csv_file_name}\")\n",
    "\n",
    "        # Ensure extraction directory exists\n",
    "        extract_dir = \"data\"\n",
    "        os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "        # Extract file (optional but useful for debugging)\n",
    "        extracted_path = zip_ref.extract(csv_file_name, path=extract_dir)\n",
    "        print(f\"üì§ Extracted to: {extracted_path}\")\n",
    "\n",
    "        # Load CSV into pandas directly from ZIP\n",
    "        with zip_ref.open(csv_file_name) as csv_file:\n",
    "            try:\n",
    "                df = pd.read_csv(csv_file)\n",
    "                print(f\"‚úÖ Loaded CSV: {csv_file_name}\")\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"‚ùå Failed to read CSV inside ZIP: {e}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51ba691",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = extract_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942b2977",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7fdc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to transform Viljoen Beverages data\n",
    "\n",
    "    Args:\n",
    "        df: Input dataframe to transform\n",
    "        returns: Transformed dataframe\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Standard column layout\n",
    "    columns=[\n",
    "        'SellerID','GUID','Date','Reference','Customer_Code','Name','Physical_Address1',\\\n",
    "        'Physical_Address2','Physical_Address3','Physical_Address4','Telephone',\\\n",
    "        'Stock_Code','Description','Price_Ex_Vat','Quantity','RepCode','ProductBarCodeID'\n",
    "        ]\n",
    "    # Create an empty dataframe\n",
    "    df1=pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Build the dataframe\n",
    "    df1['Date']=df['Date']\n",
    "    df1['SellerID']='VILJOEN'\n",
    "    df1['GUID']=0\n",
    "    df1['Reference']=df['Reference']\n",
    "    df1['Customer_Code']=df['Customer code']\n",
    "    df1['Name']=df['Customer name']\n",
    "    df1['Physical_Address1']=df['Physical_Address1']\n",
    "    df1['Physical_Address2']=df['Physical_Address2']\n",
    "    df1['Physical_Address3']=df['Physical_Address3']\n",
    "    df1['Physical_Address4']=(\n",
    "        df['Deliver1'].fillna('').astype(str) +' '+\n",
    "        df['Deliver2'].fillna('').astype(str) +' '+\n",
    "        df['Deliver3'].fillna('').astype(str) +' '+\n",
    "        df['Deliver4'].fillna('').astype(str)\n",
    "        ).str.strip()\n",
    "\n",
    "    df1['Telephone']=df['Telephone']\n",
    "    df1['Stock_Code']=df['Product code']\n",
    "    df1['Description']=df['Product description']\n",
    "    df1['Price_Ex_Vat']=round(abs(df['Value']/df['Quantity']),2)\n",
    "    df1['Quantity']=df['Quantity']\n",
    "    df1['RepCode']=df['Rep']\n",
    "    df1['ProductBarCodeID']=''\n",
    "\n",
    "    print(\"‚úÖ DATA TRANSFORMATION IN PROGRESS!!\")\n",
    "    print(f\"‚úÖ Total quantity: {np.sum(df1['Quantity']):.0f}\")\n",
    "\n",
    "    df2=df1.copy()\n",
    "    df2['Date']=pd.to_datetime(df2['Date'])\n",
    "    df2['Date']=df2['Date'].apply(lambda x: x.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "    #   INTELLIGENT NAME BACKFILLING\n",
    "    # -----------------------------------\n",
    "\n",
    "    print(\"üß† Handling missing buyer names intelligently..........\")\n",
    "    # 1) Use Customer_Code as primary matching key\n",
    "    # -----------------------------\n",
    "    df1['Name'] = df1.groupby('Customer_Code')['Name'].transform(\n",
    "        lambda x: x.fillna(x.mode().iloc[0]) if x.mode().size > 0 else x\n",
    "    )\n",
    "    # 2) Use Address fields as secondary matching key\n",
    "    # -----------------------------\n",
    "    df1['Name'] = df1.groupby(\n",
    "        ['Physical_Address1', 'Physical_Address2', 'Physical_Address3', 'Physical_Address4']\n",
    "    )['Name'].transform(\n",
    "        lambda x: x.fillna(x.mode().iloc[0]) if x.mode().size > 0 else x\n",
    "    )\n",
    "    # 3) Use telephone number as fallback\n",
    "    # -----------------------------\n",
    "    df1['Name'] = df1.groupby('Telephone')['Name'].transform(\n",
    "        lambda x: x.fillna(x.mode().iloc[0]) if x.mode().size > 0 else x\n",
    "    )\n",
    "    # 4) Global fallback (only for final unresolved missing names)\n",
    "    # -----------------------------\n",
    "    df1['Name'].fillna('SPAR NORTH RAND (11691)', inplace=True)\n",
    "    print(\"‚úÖ Missing buyer names fixed.....\")\n",
    "\n",
    "    #   DATE FORMAT CLEANING\n",
    "    # -----------------------------\n",
    "    print(\"‚úÖ Date fomat cleaned.........\")\n",
    "    df1['Date'] = pd.to_datetime(df1['Date'], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
    "    print(\"‚úÖ Data transformation complete!\")\n",
    "\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4022f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = transform_data(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e884766",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82a6483",
   "metadata": {},
   "source": [
    "## Validate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdb015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Function to validate data\n",
    "    \"\"\"\n",
    "    # logger = get_run_logger()\n",
    "    class Schema(pa.DataFrameModel):\n",
    "        # 1. Check data types and uniqueness\n",
    "        SellerID: Series[str] = pa.Field(nullable=False)  # seller IDs must be non-null\n",
    "        GUID: Series[int] = pa.Field(ge=0, nullable=False)  # must be non-null\n",
    "\n",
    "        # 2. Dates coerced to proper datetime\n",
    "        Date: Series[pd.Timestamp] = pa.Field(coerce=False, nullable=False) # must be non-null\n",
    "\n",
    "        # 3. Reference and customer codes\n",
    "        Reference: Series[str] = pa.Field(nullable=False) # must be non-null\n",
    "        Customer_Code: Series[str] = pa.Field(str_matches=r\"^[A-Z0-9]+$\", nullable=False)  # must be non-null\n",
    "\n",
    "        # 4. Customer details\n",
    "        Name: Series[str] = pa.Field(nullable=False) # must be non-null\n",
    "        Physical_Address1: Series[str] = pa.Field(nullable=True)\n",
    "        Physical_Address2: Series[str] = pa.Field(nullable=True)\n",
    "        Physical_Address3: Series[str] = pa.Field(nullable=True)\n",
    "        Physical_Address4: Series[str] = pa.Field(nullable=True)\n",
    "\n",
    "        # 5. Telephone validation (basic regex for digits, spaces, +, -)\n",
    "        Telephone: Series[str] = pa.Field(nullable=True)\n",
    "\n",
    "        # 6. Product details\n",
    "        Stock_Code: Series[str] = pa.Field(nullable=False) # must be non-null\n",
    "        Description: Series[str] = pa.Field(nullable=False) # must be non-null\n",
    "        Price_Ex_Vat: Series[float] = pa.Field(ge=0.0, nullable=False)  # must be non-null\n",
    "        Quantity: Series[int] = pa.Field(nullable=False)  # must be non-null\n",
    "\n",
    "        # 7. Rep and barcode\n",
    "        RepCode: Series[str] = pa.Field(nullable=True)\n",
    "        ProductBarCodeID: Series[str] = pa.Field(nullable=True)  # typical EAN/UPC\n",
    "\n",
    "        class Config:\n",
    "            strict = True  # enforce exact schema\n",
    "            coerce = True  # auto-convert types where possible\n",
    "\n",
    "    try:\n",
    "        # lazy=True means \"find all errors before crashing\"\n",
    "        Schema.validate(df, lazy=True)\n",
    "        print(\"‚úÖ Data passed validation! Proceeding to ETL...\")\n",
    "\n",
    "    except pa.errors.SchemaErrors as err:\n",
    "        print(\"‚ö†Ô∏è Data Contract Breached!.......\\n\")\n",
    "        print(f\"‚ùå Total errors found: {len(err.failure_cases)}\")\n",
    "\n",
    "        # Let's look at the specific failures\n",
    "        print(\"\\n*********‚ö†Ô∏èFailure Report‚ö†Ô∏è************\\n\")\n",
    "        print(err.failure_cases[['column', 'check', 'failure_case']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcac4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_data(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
